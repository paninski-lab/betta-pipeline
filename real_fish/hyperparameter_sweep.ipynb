{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 · Hyperparameter Sweep\n",
    "\n",
    "**Goal**: Perform a full hyperparameter sweep on 5 models (4 subtype-specific + 1 aggregate) using 2-class LP with calculated contour.\n",
    "\n",
    "**Hyperparameters**:\n",
    "- `num_hid_units`: 32, 64\n",
    "- `num_layers`: 2, 3\n",
    "- `num_lags`: 4, 8\n",
    "- `lr`: 1e-4, 5e-4\n",
    "- `epochs`: 400 (fixed)\n",
    "\n",
    "**Total**: 2 × 2 × 2 × 2 × 1 = 16 configurations × 5 model types = **80 models**\n",
    "\n",
    "**Output**:\n",
    "1. Best hyperparameters for each model type (selected on test data)\n",
    "2. F1 barplot with per-video points and connecting lines\n",
    "3. Flaring rate scatterplots (2 per fish type: type-specific vs aggregate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lightning_action.api import Model\n",
    "from sklearn.metrics import f1_score\n",
    "import yaml\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths & Configuration -------------------------------------------------\n",
    "PROJECT_ROOT = Path(\"C:/Columbia Capstone\")\n",
    "DATA_ROOT = PROJECT_ROOT / \"data_DLC&LP\"\n",
    "WEEK14_ROOT = PROJECT_ROOT / \"Week 14\"\n",
    "WEEK12_ROOT = PROJECT_ROOT / \"Week 12\"\n",
    "\n",
    "# Output directories\n",
    "HPARAM_ROOT = WEEK14_ROOT / \"hyperparameter_sweep\"\n",
    "HPARAM_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODELS_ROOT = HPARAM_ROOT / \"models\"\n",
    "RESULTS_ROOT = HPARAM_ROOT / \"results\"\n",
    "PLOTS_ROOT = HPARAM_ROOT / \"plots\"\n",
    "BINARY_DATA_ROOT = HPARAM_ROOT / \"binary_data\"  # Preprocessed 2-class data (Week 12 style)\n",
    "\n",
    "SEED = 43\n",
    "\n",
    "# Fixed variant: LP with calculated contour (2-class)\n",
    "VARIANT = \"LP_with_cal_contour\"\n",
    "FEATURE_COUNT = 18  # LP with calculated contour has 18 features\n",
    "OUTPUT_SIZE = 2  # 2-class (background vs flare)\n",
    "\n",
    "# Model types: 4 subtype-specific + 1 aggregate\n",
    "ALL_SUBTYPES = [\"fighting\", \"hybrid\", \"ornamental\", \"wild\"]\n",
    "MODEL_TYPES = ALL_SUBTYPES + [\"aggregate\"]\n",
    "\n",
    "# Hyperparameter grid\n",
    "HPARAM_GRID = {\n",
    "    \"num_hid_units\": [32, 64],\n",
    "    \"num_layers\": [2, 3],\n",
    "    \"num_lags\": [4, 8],\n",
    "    \"lr\": [1e-4, 5e-4],\n",
    "    \"num_epochs\": [400],  # Fixed\n",
    "}\n",
    "\n",
    "# Generate all hyperparameter combinations\n",
    "HPARAM_COMBINATIONS = list(itertools.product(\n",
    "    HPARAM_GRID[\"num_hid_units\"],\n",
    "    HPARAM_GRID[\"num_layers\"],\n",
    "    HPARAM_GRID[\"num_lags\"],\n",
    "    HPARAM_GRID[\"lr\"],\n",
    "    HPARAM_GRID[\"num_epochs\"],\n",
    "))\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Variant: {VARIANT} (2-class, {FEATURE_COUNT} features)\")\n",
    "print(f\"  Model types: {len(MODEL_TYPES)} ({', '.join(MODEL_TYPES)})\")\n",
    "print(f\"  Hyperparameter combinations: {len(HPARAM_COMBINATIONS)}\")\n",
    "print(f\"  Total models to train: {len(MODEL_TYPES) * len(HPARAM_COMBINATIONS)} = {len(MODEL_TYPES)} × {len(HPARAM_COMBINATIONS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Utility Functions\n",
    "\n",
    "Functions for dataset loading, session validation, training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions -----------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    subtype: str\n",
    "    variant: str\n",
    "    path: Path\n",
    "    feature_count: int\n",
    "    input_dir: str = \"features\"\n",
    "    feature_index: Dict[str, str] = field(default_factory=dict)\n",
    "    label_index: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def features_dir(self) -> Path:\n",
    "        return self.path / self.input_dir\n",
    "\n",
    "    @property\n",
    "    def labels_dir(self) -> Path:\n",
    "        return self.path / \"labels\"\n",
    "\n",
    "    @property\n",
    "    def split_file(self) -> Path:\n",
    "        return self.path / \"splits.yaml\"\n",
    "\n",
    "\n",
    "def normalise_filename(name: str, subtype: str) -> str:\n",
    "    \"\"\"Normalize filename to canonical form.\"\"\"\n",
    "    if name.lower().endswith(\".csv\"):\n",
    "        name = name[:-4]\n",
    "    for suffix in (\"_labels\", \"_labeled\", \"_featureonly\", \"_pure_feature\"):\n",
    "        if name.endswith(suffix):\n",
    "            name = name[: -len(suffix)]\n",
    "    \n",
    "    # Handle aggregate identifiers (e.g., \"fighting__1.2.2L\")\n",
    "    if \"__\" in name:\n",
    "        return name\n",
    "    \n",
    "    token = name.split(\"_\")[-1]\n",
    "    if subtype == \"ornamental\" and token.startswith(\"orn\"):\n",
    "        token = token[3:]\n",
    "    if subtype == \"wild\" and token.startswith(\"w\") and not token.startswith(\"wxf\"):\n",
    "        token = token[1:]\n",
    "    if subtype == \"ornamental\" and token.endswith(\"oR\"):\n",
    "        token = token[:-2] + \"R\"\n",
    "    return token\n",
    "\n",
    "\n",
    "def parse_splits(path: Path) -> Dict[str, List[str]]:\n",
    "    \"\"\"Parse splits.yaml file.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing splits file: {path}\")\n",
    "    current = None\n",
    "    splits: Dict[str, List[str]] = {\"train\": [], \"test\": [], \"val\": []}\n",
    "    for raw_line in path.read_text().splitlines():\n",
    "        line = raw_line.split(\"#\", 1)[0].strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.endswith(\":\"):\n",
    "            key = line[:-1].strip()\n",
    "            current = key if key in splits else None\n",
    "            continue\n",
    "        if current and line.startswith(\"-\"):\n",
    "            value = line[1:].strip()\n",
    "            if value:\n",
    "                splits[current].append(value)\n",
    "    return splits\n",
    "\n",
    "\n",
    "def populate_signal_indices(config: DatasetConfig) -> None:\n",
    "    \"\"\"Populate feature and label indices from disk.\"\"\"\n",
    "    if not config.feature_index:\n",
    "        for path in config.features_dir.glob(\"*.csv\"):\n",
    "            canonical = normalise_filename(path.name, config.subtype)\n",
    "            config.feature_index.setdefault(canonical, path.stem)\n",
    "    if not config.label_index:\n",
    "        for path in config.labels_dir.glob(\"*.csv\"):\n",
    "            canonical = normalise_filename(path.name, config.subtype)\n",
    "            config.label_index.setdefault(canonical, path.stem)\n",
    "\n",
    "\n",
    "def validate_sessions(config: DatasetConfig, splits: Dict[str, List[str]]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Validate that all sessions in splits have corresponding feature and label files.\"\"\"\n",
    "    populate_signal_indices(config)\n",
    "    \n",
    "    SESSION_ALIASES = {\n",
    "        \"ornamental\": {\n",
    "            \"2.7.1oR\": \"2.7.1R\",\n",
    "            \"3.5.1oR\": \"3.5.1R\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    alias_map = SESSION_ALIASES.get(config.subtype, {})\n",
    "    expected = set(splits[\"train\"]) | set(splits[\"test\"]) | set(splits[\"val\"])\n",
    "    \n",
    "    missing_features: List[str] = []\n",
    "    missing_labels: List[str] = []\n",
    "    \n",
    "    for session in expected:\n",
    "        canonical = alias_map.get(session, session)\n",
    "        # For aggregate, session might already be in format \"fighting__1.2.2L\"\n",
    "        if config.subtype == \"aggregate\" and \"__\" in session:\n",
    "            canonical = session\n",
    "        else:\n",
    "            canonical = normalise_filename(canonical, config.subtype)\n",
    "        \n",
    "        if canonical not in config.feature_index:\n",
    "            missing_features.append(session)\n",
    "        if canonical not in config.label_index:\n",
    "            missing_labels.append(session)\n",
    "    \n",
    "    return sorted(missing_features), sorted(missing_labels)\n",
    "\n",
    "\n",
    "def resolve_session_alias(session: str, subtype: str) -> str:\n",
    "    \"\"\"Resolve session alias to actual filename (e.g., '2.7.1oR' -> '2.7.1R' for ornamental).\"\"\"\n",
    "    SESSION_ALIASES = {\n",
    "        \"ornamental\": {\n",
    "            \"2.7.1oR\": \"2.7.1R\",\n",
    "            \"3.5.1oR\": \"3.5.1R\",\n",
    "        }\n",
    "    }\n",
    "    alias_map = SESSION_ALIASES.get(subtype, {})\n",
    "    return alias_map.get(session, session)\n",
    "\n",
    "\n",
    "def load_dataset_config(model_type: str) -> DatasetConfig:\n",
    "    \"\"\"Load dataset configuration for a model type.\"\"\"\n",
    "    if model_type == \"aggregate\":\n",
    "        # Aggregate dataset from Week 12\n",
    "        path = WEEK12_ROOT / \"aggregate_all\" / VARIANT\n",
    "        input_dir = \"features\"  # Aggregate uses features folder\n",
    "    else:\n",
    "        # Subtype-specific datasets\n",
    "        # Week 12 style: all subtypes read from 'features' folder\n",
    "        # The splits.yaml file determines which sessions are train/test\n",
    "        if model_type == \"fighting\":\n",
    "            base_path = DATA_ROOT / \"new_data\"\n",
    "            variant_dir = \"newLPwith_calculated_contour\"\n",
    "            input_dir = \"features\"  # Week 12 style: read from features folder\n",
    "        elif model_type == \"hybrid\":\n",
    "            base_path = DATA_ROOT / \"new_hybrid\" / \"hybrid\"\n",
    "            variant_dir = \"hybrid_LP_with_cal_contour\"\n",
    "            input_dir = \"features\"\n",
    "        elif model_type == \"ornamental\":\n",
    "            base_path = DATA_ROOT / \"new_ornamental\" / \"ornamental\"\n",
    "            variant_dir = \"ornamental_LP_with_cal_contour\"\n",
    "            input_dir = \"features\"\n",
    "        elif model_type == \"wild\":\n",
    "            base_path = DATA_ROOT / \"new_wild\" / \"wild\"\n",
    "            variant_dir = \"wild_LP_with_cal_contour\"\n",
    "            input_dir = \"features\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        path = base_path / variant_dir\n",
    "    \n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset path not found: {path}\")\n",
    "    \n",
    "    config = DatasetConfig(\n",
    "        subtype=model_type,\n",
    "        variant=VARIANT,\n",
    "        path=path,\n",
    "        feature_count=FEATURE_COUNT,\n",
    "        input_dir=input_dir,  # All subtypes use 'features' folder (Week 12 style)\n",
    "    )\n",
    "    \n",
    "    populate_signal_indices(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "def create_config_dict(\n",
    "    dataset: DatasetConfig,\n",
    "    train_sessions: List[str],\n",
    "    hparams: Tuple[int, int, int, float, int],\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Create lightning-action configuration dictionary.\n",
    "    Matches Week 12 pipeline configuration, with hyperparameters from hparams.\n",
    "    \n",
    "    Hyperparameters being searched:\n",
    "    - num_hid_units: from hparams[0]\n",
    "    - num_layers: from hparams[1]\n",
    "    - num_lags: from hparams[2]\n",
    "    - lr: from hparams[3]\n",
    "    - num_epochs: from hparams[4]\n",
    "    \n",
    "    All other parameters match Week 12 defaults.\n",
    "    \"\"\"\n",
    "    # Resolve session aliases (e.g., \"2.7.1oR\" -> \"2.7.1R\" for ornamental)\n",
    "    # For aggregate, we need to check all subtype aliases since it contains sessions from all subtypes\n",
    "    SESSION_ALIASES = {\n",
    "        \"ornamental\": {\n",
    "            \"2.7.1oR\": \"2.7.1R\",\n",
    "            \"3.5.1oR\": \"3.5.1R\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # For aggregate, check all subtype aliases; for specific subtypes, only check that subtype's aliases\n",
    "    if dataset.subtype == \"aggregate\":\n",
    "        # Merge all alias maps for aggregate\n",
    "        all_aliases = {}\n",
    "        for subtype_aliases in SESSION_ALIASES.values():\n",
    "            all_aliases.update(subtype_aliases)\n",
    "        alias_map = all_aliases\n",
    "    else:\n",
    "        alias_map = SESSION_ALIASES.get(dataset.subtype, {})\n",
    "    \n",
    "    resolved_sessions = [alias_map.get(session, session) for session in train_sessions]\n",
    "    \n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    sequence_pad = sum([2 * (2 ** n) * num_lags for n in range(num_layers)])\n",
    "    config = {\n",
    "        \"data\": {\n",
    "            \"data_path\": str(dataset.path),\n",
    "            \"input_dir\": dataset.input_dir,\n",
    "            \"transforms\": [\"ZScore\"],\n",
    "            \"expt_ids\": resolved_sessions,\n",
    "            \"seed\": SEED,\n",
    "            \"ignore_index\": -100,\n",
    "            \"weight_classes\": True,\n",
    "            \"label_names\": [\"background\", \"flare\"],\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"input_size\": dataset.feature_count,\n",
    "            \"output_size\": OUTPUT_SIZE,\n",
    "            \"backbone\": \"dtcn\",\n",
    "            \"num_hid_units\": num_hid_units,  # From hyperparameter search\n",
    "            \"num_layers\": num_layers,  # From hyperparameter search\n",
    "            \"num_lags\": num_lags,  # From hyperparameter search\n",
    "            \"sequence_pad\": sequence_pad,\n",
    "            \"seed\": SEED,\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"Adam\",\n",
    "            \"lr\": lr,  # From hyperparameter search\n",
    "            \"wd\": 0,\n",
    "            \"scheduler\": None,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"device\": \"gpu\",\n",
    "            \"num_epochs\": num_epochs,  # From hyperparameter search (400)\n",
    "            \"batch_size\": 16,  # Week 12 default\n",
    "            \"num_workers\": 4,\n",
    "            \"sequence_length\": 1000,\n",
    "            \"train_probability\": 0.95,  # Week 12 default\n",
    "            \"val_probability\": 0.05,  # Week 12 default\n",
    "        },\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def save_results(model_type: str, results: Dict) -> None:\n",
    "    \"\"\"Save results to JSON file.\"\"\"\n",
    "    results_path = RESULTS_ROOT / f\"{model_type}_results.json\"\n",
    "    results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    print(f\"✓ Results saved to {results_path}\")\n",
    "\n",
    "\n",
    "def load_results(model_type: str) -> Optional[Dict]:\n",
    "    \"\"\"Load results from JSON file if exists.\"\"\"\n",
    "    results_path = RESULTS_ROOT / f\"{model_type}_results.json\"\n",
    "    if results_path.exists():\n",
    "        with open(results_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Week 12 style: Data preprocessing functions (no monkey patches)\n",
    "def convert_labels_to_binary(label_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Convert 3-class labels to binary (flare vs background). Same as Week 12.\"\"\"\n",
    "    df = pd.read_csv(label_path)\n",
    "    columns = df.columns.tolist()\n",
    "    index_col = columns[0]\n",
    "    if {\"full_flaring\", \"half_flaring\"}.issubset(columns):\n",
    "        flare = ((df[\"full_flaring\"] > 0) | (df[\"half_flaring\"] > 0)).astype(int)\n",
    "    else:\n",
    "        class_col = columns[-1]\n",
    "        flare = (df[class_col] > 0).astype(int)\n",
    "    background = 1 - flare\n",
    "    return pd.DataFrame({index_col: df[index_col], \"background\": background, \"flare\": flare})\n",
    "\n",
    "\n",
    "def prepare_binary_dataset(cfg: DatasetConfig) -> DatasetConfig:\n",
    "    \"\"\"Prepare binary dataset from 3-class dataset.\n",
    "    统一放入 features/ + labels/，所有 subtype 一致；train/test 用 splits.yaml 控制。\n",
    "    \"\"\"\n",
    "    target_root = BINARY_DATA_ROOT / cfg.subtype / cfg.variant\n",
    "    if target_root.exists():\n",
    "        shutil.rmtree(target_root)\n",
    "    (target_root / \"features\").mkdir(parents=True, exist_ok=True)\n",
    "    (target_root / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy all feature CSVs into features/\n",
    "    # 若源有 training/test，也会被 glob(\"*.csv\") 覆盖全部；保持简单一致\n",
    "    for file in cfg.path.rglob(\"*.csv\"):\n",
    "        # 只拷贝特征，不拷贝标签；标签在下方转换写入\n",
    "        if file.parent.name.lower() == \"labels\":\n",
    "            continue\n",
    "        shutil.copy(file, target_root / \"features\" / file.name)\n",
    "\n",
    "    # Convert and save labels\n",
    "    for label_file in cfg.labels_dir.glob(\"*.csv\"):\n",
    "        binary_df = convert_labels_to_binary(label_file)\n",
    "        binary_df.to_csv(target_root / \"labels\" / label_file.name, index=False)\n",
    "\n",
    "    # Copy splits file\n",
    "    shutil.copy(cfg.split_file, target_root / \"splits.yaml\")\n",
    "\n",
    "    return DatasetConfig(\n",
    "        subtype=cfg.subtype,\n",
    "        variant=cfg.variant,\n",
    "        path=target_root,\n",
    "        feature_count=cfg.feature_count,\n",
    "        input_dir=\"features\",\n",
    "    )\n",
    "\n",
    "print(\"✓ Utility functions defined\")\n",
    "print(\"✓ Data preprocessing functions ready (Week 12 style, no monkey patches)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Binary F1 helper (Week10-style) ---------------------------------------\n",
    "\n",
    "def extract_binary_labels_and_preds(pred_df, label_df, positive_col=\"flare\"):\n",
    "    \"\"\"\n",
    "    统一把各种格式的 prediction / label DataFrame 转成 0/1（flare vs non-flare）。\n",
    "    返回: (labels, preds)，如果长度为 0 则返回 (None, None)。\n",
    "    \"\"\"\n",
    "    length = min(len(pred_df), len(label_df))\n",
    "    if length == 0:\n",
    "        return None, None\n",
    "\n",
    "    pred_df = pred_df.iloc[:length]\n",
    "    label_df = label_df.iloc[:length]\n",
    "\n",
    "    # === 预测：优先 [background, flare] → predicted → flare → 兜底一列 ===\n",
    "    if {\"background\", positive_col}.issubset(pred_df.columns):\n",
    "        scores = pred_df[[\"background\", positive_col]].values\n",
    "        preds = scores.argmax(axis=1)\n",
    "    elif \"predicted\" in pred_df.columns:\n",
    "        vals = pred_df[\"predicted\"].values\n",
    "        uniq = np.unique(vals[~pd.isna(vals)])\n",
    "        if set(uniq).issubset({0, 1}):\n",
    "            preds = vals.astype(int)\n",
    "        else:\n",
    "            preds = (vals > 0).astype(int)\n",
    "    elif positive_col in pred_df.columns:\n",
    "        scores = pred_df[positive_col].values\n",
    "        min_v, max_v = np.nanmin(scores), np.nanmax(scores)\n",
    "        thresh = 0.5 if (0.0 <= min_v <= 1.0 and 0.0 <= max_v <= 1.0) else 0.0\n",
    "        preds = (scores > thresh).astype(int)\n",
    "    else:\n",
    "        scores = pred_df.iloc[:, -1].values\n",
    "        min_v, max_v = np.nanmin(scores), np.nanmax(scores)\n",
    "        thresh = 0.5 if (0.0 <= min_v <= 1.0 and 0.0 <= max_v <= 1.0) else 0.0\n",
    "        preds = (scores > thresh).astype(int)\n",
    "\n",
    "    # === 标签：优先 [background, flare] → flare → 兜底一列 ===\n",
    "    if {\"background\", positive_col}.issubset(label_df.columns):\n",
    "        scores = label_df[[\"background\", positive_col]].values\n",
    "        labels = scores.argmax(axis=1)\n",
    "    elif positive_col in label_df.columns:\n",
    "        labels = (label_df[positive_col].values > 0).astype(int)\n",
    "    else:\n",
    "        labels = (label_df.iloc[:, -1].values > 0).astype(int)\n",
    "\n",
    "    return labels, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Validate Datasets\n",
    "\n",
    "Load all dataset configurations and validate that all sessions are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dataset configurations\n",
    "dataset_configs = {}\n",
    "\n",
    "for model_type in MODEL_TYPES:\n",
    "    try:\n",
    "        config = load_dataset_config(model_type)\n",
    "        splits = parse_splits(config.split_file)\n",
    "        missing_feat, missing_lab = validate_sessions(config, splits)\n",
    "        \n",
    "        if missing_feat or missing_lab:\n",
    "            print(f\"⚠ {model_type:12s}: Missing features={missing_feat}, Missing labels={missing_lab}\")\n",
    "        else:\n",
    "            print(f\"✓ {model_type:12s}: {len(splits['train'])} train, {len(splits['test'])} test, {len(splits['val'])} val\")\n",
    "            dataset_configs[model_type] = config\n",
    "            \n",
    "            # Check data file length consistency for first few sessions\n",
    "            if model_type == \"fighting\":  # Only check fighting for now\n",
    "                print(f\"\\n  Checking data file lengths for {model_type}...\")\n",
    "                all_sessions = splits['train'] + splits['test']\n",
    "                for session in all_sessions[:3]:  # Check first 3 sessions\n",
    "                    try:\n",
    "                        feat_file = config.features_dir / f\"{session}.csv\"\n",
    "                        label_file = config.labels_dir / f\"{session}.csv\"\n",
    "                        if not feat_file.exists():\n",
    "                            canonical = normalise_filename(session, model_type)\n",
    "                            feat_stem = config.feature_index.get(canonical, canonical)\n",
    "                            feat_file = config.features_dir / f\"{feat_stem}.csv\"\n",
    "                        if not label_file.exists():\n",
    "                            canonical = normalise_filename(session, model_type)\n",
    "                            label_stem = config.label_index.get(canonical, canonical)\n",
    "                            label_file = config.labels_dir / f\"{label_stem}.csv\"\n",
    "                        \n",
    "                        if feat_file.exists() and label_file.exists():\n",
    "                            feat_df = pd.read_csv(feat_file, index_col=0)\n",
    "                            label_df = pd.read_csv(label_file, index_col=0)\n",
    "                            match = feat_df.shape[0] == label_df.shape[0]\n",
    "                            status = \"✓\" if match else \"✗\"\n",
    "                            print(f\"    {status} {session:15s}: feat={feat_df.shape[0]:6d}, label={label_df.shape[0]:6d}, match={match}\")\n",
    "                            \n",
    "                            # Additional check: verify sequence count calculation\n",
    "                            if not match:\n",
    "                                seq_len = 1000\n",
    "                                feat_seqs = int(feat_df.shape[0] / seq_len)\n",
    "                                label_seqs = int(label_df.shape[0] / seq_len)\n",
    "                                print(f\"      -> feat sequences: {feat_seqs}, label sequences: {label_seqs}\")\n",
    "                                print(f\"      -> feat remainder: {feat_df.shape[0] % seq_len}, label remainder: {label_df.shape[0] % seq_len}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ✗ {session:15s}: Error checking - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {model_type:12s}: Error - {e}\")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(dataset_configs)} dataset configurations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training - Subtype-Specific Models\n",
    "\n",
    "Each subtype has 2 cells:\n",
    "1. **Training Cell**: Train all 16 hyperparameter combinations\n",
    "2. **Inference Cell**: Run inference on test sessions and evaluate\n",
    "\n",
    "### Fighting Subtype - Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_binary_metrics_per_session(predictions_dir, dataset_config, test_sessions):\n",
    "    \"\"\"Calculate flare/background F1 scores for binary models. (Week10 style)\"\"\"\n",
    "    results = []\n",
    "    data_path = Path(dataset_config[\"data_path\"])\n",
    "    labels_dir = data_path / \"labels\"\n",
    "\n",
    "    for session in test_sessions:\n",
    "        pred_file = predictions_dir / f\"{session}_predictions.csv\"\n",
    "        if not pred_file.exists():\n",
    "            print(f\"  ⚠️  Prediction file not found for session {session}\")\n",
    "            continue\n",
    "\n",
    "        gt_file = labels_dir / f\"{session}.csv\"\n",
    "        if not gt_file.exists():\n",
    "            print(f\"  ⚠️  Ground truth file not found for session {session}\")\n",
    "            continue\n",
    "\n",
    "        pred_df = pd.read_csv(pred_file)\n",
    "        gt_df = pd.read_csv(gt_file)\n",
    "\n",
    "        # 对齐长度\n",
    "        min_len = min(len(pred_df), len(gt_df))\n",
    "        pred_df = pred_df.iloc[:min_len]\n",
    "        gt_df = gt_df.iloc[:min_len]\n",
    "\n",
    "        # 预测 → 0/1\n",
    "        if {\"background\", \"flare\"}.issubset(pred_df.columns):\n",
    "            predictions = np.argmax(pred_df[[\"background\", \"flare\"]].values, axis=1)\n",
    "        elif \"predicted\" in pred_df.columns:\n",
    "            predictions = (pred_df[\"predicted\"].astype(int) > 0).astype(int).values\n",
    "        else:\n",
    "            print(f\"  ⚠️  Unknown prediction format for {session}\")\n",
    "            continue\n",
    "\n",
    "        # 标签 → 0/1\n",
    "        if {\"background\", \"flare\"}.issubset(gt_df.columns):\n",
    "            ground_truth = np.argmax(gt_df[[\"background\", \"flare\"]].values, axis=1)\n",
    "        else:\n",
    "            ground_truth = (gt_df.iloc[:, -1].astype(int) > 0).astype(int).values\n",
    "\n",
    "        f1_flare = f1_score(ground_truth, predictions, average=\"binary\")\n",
    "        flare_rate = ground_truth.mean()\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"session\": session,\n",
    "                \"flare_f1\": f1_flare,\n",
    "                \"flare_rate\": flare_rate,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fighting Subtype - Training\n",
    "# Train all 16 hyperparameter combinations\n",
    "\n",
    "MODEL_TYPE = \"fighting\"\n",
    "\n",
    "# Load previous results if exists (allows resuming after crash)\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Preprocess data to 2-class before training\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "print(f\"Preprocessing {MODEL_TYPE} dataset to 2-class (Week 12 style)...\")\n",
    "binary_dataset = prepare_binary_dataset(raw_dataset)\n",
    "print(f\"✓ Preprocessed data saved to: {binary_dataset.path}\")\n",
    "\n",
    "splits = parse_splits(binary_dataset.split_file)\n",
    "dataset = binary_dataset  # Use preprocessed dataset for training\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRAINING: {MODEL_TYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Train sessions: {len(splits['train'])}\")\n",
    "print(f\"Test sessions: {len(splits['test'])}\")\n",
    "print(f\"Hyperparameter combinations: {len(HPARAM_COMBINATIONS)}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already trained\n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    if (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already trained\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Training...\")\n",
    "    \n",
    "    try:\n",
    "        config = create_config_dict(dataset, splits[\"train\"], hparams)\n",
    "        \n",
    "        # Save config\n",
    "        config_path = model_dir / \"config.yaml\"\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(config_path, \"w\") as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        \n",
    "        # Train model\n",
    "        # Week 10 style: pass config file path instead of dict\n",
    "        # This ensures Model.from_config loads the config fresh and handles sequence_pad correctly\n",
    "        model = Model.from_config(str(config_path))\n",
    "        model.train(output_dir=str(model_dir))\n",
    "        \n",
    "        print(f\"    ✓ Training completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Training failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save progress\n",
    "results[\"training_completed\"] = True\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fighting Subtype - Inference & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fighting Subtype - Inference & Evaluation\n",
    "# Run inference on test sessions and compute F1 scores\n",
    "\n",
    "MODEL_TYPE = \"fighting\"\n",
    "\n",
    "# Load previous results\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "results.setdefault(\"hparam_results\", {})\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Use preprocessed 2-class data\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "binary_dataset = DatasetConfig(\n",
    "    subtype=raw_dataset.subtype,\n",
    "    variant=raw_dataset.variant,\n",
    "    path=BINARY_DATA_ROOT / raw_dataset.subtype / raw_dataset.variant,\n",
    "    feature_count=raw_dataset.feature_count,\n",
    "    input_dir=\"features\",\n",
    ")\n",
    "populate_signal_indices(binary_dataset)\n",
    "\n",
    "dataset = binary_dataset  # Use preprocessed dataset for inference\n",
    "splits = parse_splits(dataset.split_file)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"INFERENCE & EVALUATION: {MODEL_TYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test sessions: {len(splits['test'])}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    \n",
    "    # Skip if already evaluated\n",
    "    if hparam_key in results[\"hparam_results\"]:\n",
    "        f1 = results[\"hparam_results\"][hparam_key].get(\"test_f1\", \"N/A\")\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already evaluated (F1={f1})\")\n",
    "        continue\n",
    "    \n",
    "    if not (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✗ Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Running inference...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = Model.from_dir(str(model_dir))\n",
    "        \n",
    "        # Run predictions\n",
    "        pred_dir = model_dir / \"predictions\"\n",
    "        pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for session in splits[\"test\"]:\n",
    "            model.predict(\n",
    "                data_path=str(dataset.path),\n",
    "                input_dir=dataset.input_dir,\n",
    "                output_dir=str(pred_dir),\n",
    "                expt_ids=[session],\n",
    "            )\n",
    "        \n",
    "        # Load predictions and compute F1\n",
    "        from sklearn.metrics import f1_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        per_session_f1 = {}\n",
    "        \n",
    "        for session in splits[\"test\"]:\n",
    "            # Find prediction file\n",
    "            pred_file = None\n",
    "            for pattern in [f\"{session}_predictions.csv\", f\"{session}.csv\"]:\n",
    "                candidate = pred_dir / pattern\n",
    "                if candidate.exists():\n",
    "                    pred_file = candidate\n",
    "                    break\n",
    "            \n",
    "            if pred_file is None:\n",
    "                canonical = normalise_filename(session, MODEL_TYPE)\n",
    "                feat_stem = dataset.feature_index.get(canonical, canonical)\n",
    "                pred_file = pred_dir / f\"{feat_stem}_predictions.csv\"\n",
    "            \n",
    "            label_file = dataset.labels_dir / f\"{session}.csv\"\n",
    "            if not label_file.exists():\n",
    "                canonical = normalise_filename(session, MODEL_TYPE)\n",
    "                label_stem = dataset.label_index.get(canonical, canonical)\n",
    "                label_file = dataset.labels_dir / f\"{label_stem}.csv\"\n",
    "            \n",
    "            if not pred_file.exists() or not label_file.exists():\n",
    "                continue\n",
    "            \n",
    "            pred_df = pd.read_csv(pred_file)\n",
    "            label_df = pd.read_csv(label_file)\n",
    "            \n",
    "            # 统一用 Week10 风格把 CSV 转成 0/1，再算 F1\n",
    "            labels, preds = extract_binary_labels_and_preds(pred_df, label_df, positive_col=\"flare\")\n",
    "            if labels is None:\n",
    "                continue\n",
    "            \n",
    "            session_f1 = f1_score(labels, preds, average=\"binary\")\n",
    "            per_session_f1[session] = float(session_f1)\n",
    "        \n",
    "        # Overall F1 (per-session mean of binary F1)\n",
    "        if per_session_f1:\n",
    "            overall_f1 = float(np.nanmean(list(per_session_f1.values())))\n",
    "        else:\n",
    "            overall_f1 = float(\"nan\")\n",
    "        \n",
    "        results[\"hparam_results\"][hparam_key] = {\n",
    "            \"hparams\": {\n",
    "                \"num_hid_units\": int(num_hid_units),\n",
    "                \"num_layers\": int(num_layers),\n",
    "                \"num_lags\": int(num_lags),\n",
    "                \"lr\": float(lr),\n",
    "                \"num_epochs\": int(num_epochs),\n",
    "            },\n",
    "            \"test_f1\": float(overall_f1),\n",
    "            \"per_session_f1\": per_session_f1,\n",
    "            \"model_dir\": str(model_dir),\n",
    "        }\n",
    "        \n",
    "        print(f\"    ✓ F1 = {overall_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Inference & evaluation completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Subtype - Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Subtype - Training\n",
    "# Train all 16 hyperparameter combinations\n",
    "\n",
    "MODEL_TYPE = \"hybrid\"\n",
    "\n",
    "# Load previous results if exists (allows resuming after crash)\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Preprocess data to 2-class before training\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "print(f\"Preprocessing {MODEL_TYPE} dataset to 2-class (Week 12 style)...\")\n",
    "binary_dataset = prepare_binary_dataset(raw_dataset)\n",
    "print(f\"✓ Preprocessed data saved to: {binary_dataset.path}\")\n",
    "\n",
    "splits = parse_splits(binary_dataset.split_file)\n",
    "dataset = binary_dataset  # Use preprocessed dataset for training\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRAINING: {MODEL_TYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Train sessions: {len(splits['train'])}\")\n",
    "print(f\"Test sessions: {len(splits['test'])}\")\n",
    "print(f\"Hyperparameter combinations: {len(HPARAM_COMBINATIONS)}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already trained\n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    if (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already trained\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Training...\")\n",
    "    \n",
    "    try:\n",
    "        config = create_config_dict(dataset, splits[\"train\"], hparams)\n",
    "        \n",
    "        # Save config\n",
    "        config_path = model_dir / \"config.yaml\"\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(config_path, \"w\") as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        \n",
    "        # Train model\n",
    "        # Week 10/12 style: pass config file path instead of dict\n",
    "        # This ensures Model.from_config loads the config fresh and handles sequence_pad correctly\n",
    "        model = Model.from_config(str(config_path))\n",
    "        model.train(output_dir=str(model_dir))\n",
    "        \n",
    "        print(f\"    ✓ Training completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Training failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save progress\n",
    "results[\"training_completed\"] = True\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Subtype - Inference & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Subtype - Inference & Evaluation\n",
    "# Run inference on test sessions and compute F1 scores\n",
    "\n",
    "MODEL_TYPE = \"hybrid\"\n",
    "\n",
    "# Load previous results\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "results.setdefault(\"hparam_results\", {})\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Use preprocessed 2-class data\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "binary_dataset = DatasetConfig(\n",
    "    subtype=raw_dataset.subtype,\n",
    "    variant=raw_dataset.variant,\n",
    "    path=BINARY_DATA_ROOT / raw_dataset.subtype / raw_dataset.variant,\n",
    "    feature_count=raw_dataset.feature_count,\n",
    "    input_dir=\"features\",\n",
    ")\n",
    "populate_signal_indices(binary_dataset)\n",
    "\n",
    "dataset = binary_dataset  # Use preprocessed dataset for inference\n",
    "splits = parse_splits(dataset.split_file)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"INFERENCE & EVALUATION: {MODEL_TYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test sessions: {len(splits['test'])}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    \n",
    "    # Skip if already evaluated\n",
    "    if hparam_key in results[\"hparam_results\"]:\n",
    "        f1 = results[\"hparam_results\"][hparam_key].get(\"test_f1\", \"N/A\")\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already evaluated (F1={f1})\")\n",
    "        continue\n",
    "    \n",
    "    if not (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✗ Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Running inference...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = Model.from_dir(str(model_dir))\n",
    "        \n",
    "        # Run predictions\n",
    "        pred_dir = model_dir / \"predictions\"\n",
    "        pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for session in splits[\"test\"]:\n",
    "            model.predict(\n",
    "                data_path=str(dataset.path),\n",
    "                input_dir=dataset.input_dir,\n",
    "                output_dir=str(pred_dir),\n",
    "                expt_ids=[session],\n",
    "            )\n",
    "        \n",
    "        # Load predictions and compute F1\n",
    "        from sklearn.metrics import f1_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        per_session_f1 = {}\n",
    "        \n",
    "        for session in splits[\"test\"]:\n",
    "            # Find prediction file\n",
    "            pred_file = None\n",
    "            for pattern in [f\"{session}_predictions.csv\", f\"{session}.csv\"]:\n",
    "                candidate = pred_dir / pattern\n",
    "                if candidate.exists():\n",
    "                    pred_file = candidate\n",
    "                    break\n",
    "            \n",
    "            if pred_file is None:\n",
    "                canonical = normalise_filename(session, MODEL_TYPE)\n",
    "                feat_stem = dataset.feature_index.get(canonical, canonical)\n",
    "                pred_file = pred_dir / f\"{feat_stem}_predictions.csv\"\n",
    "            \n",
    "            label_file = dataset.labels_dir / f\"{session}.csv\"\n",
    "            if not label_file.exists():\n",
    "                canonical = normalise_filename(session, MODEL_TYPE)\n",
    "                label_stem = dataset.label_index.get(canonical, canonical)\n",
    "                label_file = dataset.labels_dir / f\"{label_stem}.csv\"\n",
    "            \n",
    "            if not pred_file.exists() or not label_file.exists():\n",
    "                continue\n",
    "            \n",
    "            pred_df = pd.read_csv(pred_file)\n",
    "            label_df = pd.read_csv(label_file)\n",
    "            \n",
    "            length = min(len(pred_df), len(label_df))\n",
    "            if length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract predictions (2-class)\n",
    "            if \"predicted\" in pred_df.columns:\n",
    "                preds = pred_df[\"predicted\"].astype(int).values[:length]\n",
    "            elif \"flare\" in pred_df.columns:\n",
    "                preds = (pred_df[\"flare\"] > 0.5).astype(int).values[:length]\n",
    "            else:\n",
    "                preds = pred_df.iloc[:length, -1].astype(int).values\n",
    "            \n",
    "            # Extract labels (2-class)\n",
    "            if \"flare\" in label_df.columns:\n",
    "                labels = (label_df[\"flare\"] > 0).astype(int).values[:length]\n",
    "            else:\n",
    "                labels = (label_df.iloc[:length, -1] > 0).astype(int).values\n",
    "            \n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            # Per-session F1 (binary for 2-class: flare vs non-flare)\n",
    "            if labels.size > 0:\n",
    "                session_f1 = f1_score(labels, preds, average=\"binary\")\n",
    "                per_session_f1[session] = float(session_f1)\n",
    "        \n",
    "        # Overall F1 (per-session mean of binary F1)\n",
    "        if per_session_f1:\n",
    "            overall_f1 = float(np.nanmean(list(per_session_f1.values())))\n",
    "        else:\n",
    "            overall_f1 = float(\"nan\")\n",
    "        \n",
    "        results[\"hparam_results\"][hparam_key] = {\n",
    "            \"hparams\": {\n",
    "                \"num_hid_units\": int(num_hid_units),\n",
    "                \"num_layers\": int(num_layers),\n",
    "                \"num_lags\": int(num_lags),\n",
    "                \"lr\": float(lr),\n",
    "                \"num_epochs\": int(num_epochs),\n",
    "            },\n",
    "            \"test_f1\": float(overall_f1),\n",
    "            \"per_session_f1\": per_session_f1,\n",
    "            \"model_dir\": str(model_dir),\n",
    "        }\n",
    "        \n",
    "        print(f\"    ✓ F1 = {overall_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Inference & evaluation completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ornamental Subtype - Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornamental Subtype - Training\n",
    "# Train all 16 hyperparameter combinations\n",
    "\n",
    "MODEL_TYPE = \"ornamental\"\n",
    "\n",
    "# Load previous results if exists (allows resuming after crash)\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Preprocess data to 2-class before training\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "print(f\"Preprocessing {MODEL_TYPE} dataset to 2-class (Week 12 style)...\")\n",
    "binary_dataset = prepare_binary_dataset(raw_dataset)\n",
    "print(f\"✓ Preprocessed data saved to: {binary_dataset.path}\")\n",
    "\n",
    "splits = parse_splits(binary_dataset.split_file)\n",
    "dataset = binary_dataset  # Use preprocessed dataset for training\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRAINING: {MODEL_TYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Train sessions: {len(splits['train'])}\")\n",
    "print(f\"Test sessions: {len(splits['test'])}\")\n",
    "print(f\"Hyperparameter combinations: {len(HPARAM_COMBINATIONS)}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already trained\n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    if (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already trained\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Training...\")\n",
    "    \n",
    "    try:\n",
    "        config = create_config_dict(dataset, splits[\"train\"], hparams)\n",
    "        \n",
    "        # Save config\n",
    "        config_path = model_dir / \"config.yaml\"\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(config_path, \"w\") as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        \n",
    "        # Train model\n",
    "        # Week 10/12 style: pass config file path instead of dict\n",
    "        # This ensures Model.from_config loads the config fresh and handles sequence_pad correctly\n",
    "        model = Model.from_config(str(config_path))\n",
    "        model.train(output_dir=str(model_dir))\n",
    "        \n",
    "        print(f\"    ✓ Training completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Training failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save progress\n",
    "results[\"training_completed\"] = True\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ornamental Subtype - Inference & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornamental Subtype - Inference & Evaluation\n",
    "# Run inference on test sessions and compute F1 scores\n",
    "\n",
    "MODEL_TYPE = \"ornamental\"\n",
    "\n",
    "# Load previous results\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "results.setdefault(\"hparam_results\", {})\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Use preprocessed 2-class data\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "binary_dataset = DatasetConfig(\n",
    "    subtype=raw_dataset.subtype,\n",
    "    variant=raw_dataset.variant,\n",
    "    path=BINARY_DATA_ROOT / raw_dataset.subtype / raw_dataset.variant,\n",
    "    feature_count=raw_dataset.feature_count,\n",
    "    input_dir=\"features\",\n",
    ")\n",
    "populate_signal_indices(binary_dataset)\n",
    "\n",
    "dataset = binary_dataset  # Use preprocessed dataset for inference\n",
    "splits = parse_splits(dataset.split_file)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"INFERENCE & EVALUATION: {MODEL_TYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test sessions: {len(splits['test'])}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    \n",
    "    # Skip if already evaluated\n",
    "    if hparam_key in results[\"hparam_results\"]:\n",
    "        f1 = results[\"hparam_results\"][hparam_key].get(\"test_f1\", \"N/A\")\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already evaluated (F1={f1})\")\n",
    "        continue\n",
    "    \n",
    "    if not (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✗ Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Running inference...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = Model.from_dir(str(model_dir))\n",
    "        \n",
    "        # Run predictions\n",
    "        pred_dir = model_dir / \"predictions\"\n",
    "        pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for session in splits[\"test\"]:\n",
    "            # Resolve session alias to actual filename (e.g., \"2.7.1oR\" -> \"2.7.1R\")\n",
    "            resolved_session = resolve_session_alias(session, MODEL_TYPE)\n",
    "            model.predict(\n",
    "                data_path=str(dataset.path),\n",
    "                input_dir=dataset.input_dir,\n",
    "                output_dir=str(pred_dir),\n",
    "                expt_ids=[resolved_session],\n",
    "            )\n",
    "        \n",
    "        # Load predictions and compute F1\n",
    "        from sklearn.metrics import f1_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        per_session_f1 = {}\n",
    "        \n",
    "        for session in splits[\"test\"]:\n",
    "            # Find prediction file\n",
    "            pred_file = None\n",
    "            for pattern in [f\"{session}_predictions.csv\", f\"{session}.csv\"]:\n",
    "                candidate = pred_dir / pattern\n",
    "                if candidate.exists():\n",
    "                    pred_file = candidate\n",
    "                    break\n",
    "            \n",
    "            if pred_file is None:\n",
    "                canonical = normalise_filename(session, MODEL_TYPE)\n",
    "                feat_stem = dataset.feature_index.get(canonical, canonical)\n",
    "                pred_file = pred_dir / f\"{feat_stem}_predictions.csv\"\n",
    "            \n",
    "            label_file = dataset.labels_dir / f\"{session}.csv\"\n",
    "            if not label_file.exists():\n",
    "                canonical = normalise_filename(session, MODEL_TYPE)\n",
    "                label_stem = dataset.label_index.get(canonical, canonical)\n",
    "                label_file = dataset.labels_dir / f\"{label_stem}.csv\"\n",
    "            \n",
    "            if not pred_file.exists() or not label_file.exists():\n",
    "                continue\n",
    "            \n",
    "            pred_df = pd.read_csv(pred_file)\n",
    "            label_df = pd.read_csv(label_file)\n",
    "            \n",
    "            length = min(len(pred_df), len(label_df))\n",
    "            if length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract predictions (2-class)\n",
    "            if \"predicted\" in pred_df.columns:\n",
    "                preds = pred_df[\"predicted\"].astype(int).values[:length]\n",
    "            elif \"flare\" in pred_df.columns:\n",
    "                preds = (pred_df[\"flare\"] > 0.5).astype(int).values[:length]\n",
    "            else:\n",
    "                preds = pred_df.iloc[:length, -1].astype(int).values\n",
    "            \n",
    "            # Extract labels (2-class)\n",
    "            if \"flare\" in label_df.columns:\n",
    "                labels = (label_df[\"flare\"] > 0).astype(int).values[:length]\n",
    "            else:\n",
    "                labels = (label_df.iloc[:length, -1] > 0).astype(int).values\n",
    "            \n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            # Per-session F1 (binary for 2-class: flare vs non-flare)\n",
    "            if labels.size > 0:\n",
    "                session_f1 = f1_score(labels, preds, average=\"binary\")\n",
    "                per_session_f1[session] = float(session_f1)\n",
    "        \n",
    "        # Overall F1 (per-session mean of binary F1)\n",
    "        if per_session_f1:\n",
    "            overall_f1 = float(np.nanmean(list(per_session_f1.values())))\n",
    "        else:\n",
    "            overall_f1 = float(\"nan\")\n",
    "        \n",
    "        results[\"hparam_results\"][hparam_key] = {\n",
    "            \"hparams\": {\n",
    "                \"num_hid_units\": int(num_hid_units),\n",
    "                \"num_layers\": int(num_layers),\n",
    "                \"num_lags\": int(num_lags),\n",
    "                \"lr\": float(lr),\n",
    "                \"num_epochs\": int(num_epochs),\n",
    "            },\n",
    "            \"test_f1\": float(overall_f1),\n",
    "            \"per_session_f1\": per_session_f1,\n",
    "            \"model_dir\": str(model_dir),\n",
    "        }\n",
    "        \n",
    "        print(f\"    ✓ F1 = {overall_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Inference & evaluation completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wild Subtype - Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wild Subtype - Training\n",
    "# Train all 16 hyperparameter combinations\n",
    "\n",
    "MODEL_TYPE = \"wild\"\n",
    "\n",
    "# Load previous results if exists (allows resuming after crash)\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Preprocess data to 2-class before training\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "print(f\"Preprocessing {MODEL_TYPE} dataset to 2-class (Week 12 style)...\")\n",
    "binary_dataset = prepare_binary_dataset(raw_dataset)\n",
    "print(f\"✓ Preprocessed data saved to: {binary_dataset.path}\")\n",
    "\n",
    "splits = parse_splits(binary_dataset.split_file)\n",
    "dataset = binary_dataset  # Use preprocessed dataset for training\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRAINING: {MODEL_TYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Train sessions: {len(splits['train'])}\")\n",
    "print(f\"Test sessions: {len(splits['test'])}\")\n",
    "print(f\"Hyperparameter combinations: {len(HPARAM_COMBINATIONS)}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already trained\n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    if (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already trained\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Training...\")\n",
    "    \n",
    "    try:\n",
    "        config = create_config_dict(dataset, splits[\"train\"], hparams)\n",
    "        \n",
    "        # Save config\n",
    "        config_path = model_dir / \"config.yaml\"\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(config_path, \"w\") as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        \n",
    "        # Train model\n",
    "        # Week 10/12 style: pass config file path instead of dict\n",
    "        # This ensures Model.from_config loads the config fresh and handles sequence_pad correctly\n",
    "        model = Model.from_config(str(config_path))\n",
    "        model.train(output_dir=str(model_dir))\n",
    "        \n",
    "        print(f\"    ✓ Training completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Training failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save progress\n",
    "results[\"training_completed\"] = True\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wild Subtype - Inference & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wild Subtype - Inference & Evaluation\n",
    "# Run inference on test sessions and compute F1 scores\n",
    "\n",
    "MODEL_TYPE = \"wild\"\n",
    "\n",
    "# Load previous results\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "results.setdefault(\"hparam_results\", {})\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Use preprocessed 2-class data\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "binary_dataset = DatasetConfig(\n",
    "    subtype=raw_dataset.subtype,\n",
    "    variant=raw_dataset.variant,\n",
    "    path=BINARY_DATA_ROOT / raw_dataset.subtype / raw_dataset.variant,\n",
    "    feature_count=raw_dataset.feature_count,\n",
    "    input_dir=\"features\",\n",
    ")\n",
    "populate_signal_indices(binary_dataset)\n",
    "\n",
    "dataset = binary_dataset  # Use preprocessed dataset for inference\n",
    "splits = parse_splits(dataset.split_file)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"INFERENCE & EVALUATION: {MODEL_TYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test sessions: {len(splits['test'])}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    \n",
    "    # Skip if already evaluated\n",
    "    if hparam_key in results[\"hparam_results\"]:\n",
    "        f1 = results[\"hparam_results\"][hparam_key].get(\"test_f1\", \"N/A\")\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already evaluated (F1={f1})\")\n",
    "        continue\n",
    "    \n",
    "    if not (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✗ Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Running inference...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = Model.from_dir(str(model_dir))\n",
    "        \n",
    "        # Run predictions\n",
    "        pred_dir = model_dir / \"predictions\"\n",
    "        pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for session in splits[\"test\"]:\n",
    "            model.predict(\n",
    "                data_path=str(dataset.path),\n",
    "                input_dir=dataset.input_dir,\n",
    "                output_dir=str(pred_dir),\n",
    "                expt_ids=[session],\n",
    "            )\n",
    "        \n",
    "        # Load predictions and compute F1\n",
    "        from sklearn.metrics import f1_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        per_session_f1 = {}\n",
    "        \n",
    "        for session in splits[\"test\"]:\n",
    "            # Find prediction file\n",
    "            pred_file = None\n",
    "            for pattern in [f\"{session}_predictions.csv\", f\"{session}.csv\"]:\n",
    "                candidate = pred_dir / pattern\n",
    "                if candidate.exists():\n",
    "                    pred_file = candidate\n",
    "                    break\n",
    "            \n",
    "            if pred_file is None:\n",
    "                canonical = normalise_filename(session, MODEL_TYPE)\n",
    "                feat_stem = dataset.feature_index.get(canonical, canonical)\n",
    "                pred_file = pred_dir / f\"{feat_stem}_predictions.csv\"\n",
    "            \n",
    "            label_file = dataset.labels_dir / f\"{session}.csv\"\n",
    "            if not label_file.exists():\n",
    "                canonical = normalise_filename(session, MODEL_TYPE)\n",
    "                label_stem = dataset.label_index.get(canonical, canonical)\n",
    "                label_file = dataset.labels_dir / f\"{label_stem}.csv\"\n",
    "            \n",
    "            if not pred_file.exists() or not label_file.exists():\n",
    "                continue\n",
    "            \n",
    "            pred_df = pd.read_csv(pred_file)\n",
    "            label_df = pd.read_csv(label_file)\n",
    "            \n",
    "            length = min(len(pred_df), len(label_df))\n",
    "            if length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract predictions (2-class)\n",
    "            if \"predicted\" in pred_df.columns:\n",
    "                preds = pred_df[\"predicted\"].astype(int).values[:length]\n",
    "            elif \"flare\" in pred_df.columns:\n",
    "                preds = (pred_df[\"flare\"] > 0.5).astype(int).values[:length]\n",
    "            else:\n",
    "                preds = pred_df.iloc[:length, -1].astype(int).values\n",
    "            \n",
    "            # Extract labels (2-class)\n",
    "            if \"flare\" in label_df.columns:\n",
    "                labels = (label_df[\"flare\"] > 0).astype(int).values[:length]\n",
    "            else:\n",
    "                labels = (label_df.iloc[:length, -1] > 0).astype(int).values\n",
    "            \n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            # Per-session F1 (binary for 2-class: flare vs non-flare)\n",
    "            if labels.size > 0:\n",
    "                session_f1 = f1_score(labels, preds, average=\"binary\")\n",
    "                per_session_f1[session] = float(session_f1)\n",
    "        \n",
    "        # Overall F1 (per-session mean of binary F1)\n",
    "        if per_session_f1:\n",
    "            overall_f1 = float(np.nanmean(list(per_session_f1.values())))\n",
    "        else:\n",
    "            overall_f1 = float(\"nan\")\n",
    "        \n",
    "        results[\"hparam_results\"][hparam_key] = {\n",
    "            \"hparams\": {\n",
    "                \"num_hid_units\": int(num_hid_units),\n",
    "                \"num_layers\": int(num_layers),\n",
    "                \"num_lags\": int(num_lags),\n",
    "                \"lr\": float(lr),\n",
    "                \"num_epochs\": int(num_epochs),\n",
    "            },\n",
    "            \"test_f1\": float(overall_f1),\n",
    "            \"per_session_f1\": per_session_f1,\n",
    "            \"model_dir\": str(model_dir),\n",
    "        }\n",
    "        \n",
    "        print(f\"    ✓ F1 = {overall_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Inference & evaluation completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training - Aggregate Model\n",
    "\n",
    "The aggregate model trains on ALL subtypes' training sessions, then we'll run inference separately on each subtype's test sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Model - Training\n",
    "# Train on ALL subtypes' training sessions\n",
    "\n",
    "MODEL_TYPE = \"aggregate\"\n",
    "\n",
    "# Load previous results if exists\n",
    "results = load_results(MODEL_TYPE) or {}\n",
    "\n",
    "if MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {MODEL_TYPE}\")\n",
    "\n",
    "# Week 12 style: Preprocess data to 2-class before training\n",
    "raw_dataset = dataset_configs[MODEL_TYPE]\n",
    "print(f\"Preprocessing {MODEL_TYPE} dataset to 2-class (Week 12 style)...\")\n",
    "binary_dataset = prepare_binary_dataset(raw_dataset)\n",
    "print(f\"✓ Preprocessed data saved to: {binary_dataset.path}\")\n",
    "\n",
    "splits = parse_splits(binary_dataset.split_file)\n",
    "dataset = binary_dataset  # Use preprocessed dataset for training\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRAINING: {MODEL_TYPE.upper()} MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Train sessions: {len(splits['train'])} (from all subtypes)\")\n",
    "print(f\"Test sessions: {len(splits['test'])} (from all subtypes)\")\n",
    "print(f\"Hyperparameter combinations: {len(HPARAM_COMBINATIONS)}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already trained\n",
    "    model_dir = MODELS_ROOT / MODEL_TYPE / hparam_key\n",
    "    if (model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already trained\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Training...\")\n",
    "    \n",
    "    try:\n",
    "        config = create_config_dict(dataset, splits[\"train\"], hparams)\n",
    "        \n",
    "        # Save config\n",
    "        config_path = model_dir / \"config.yaml\"\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(config_path, \"w\") as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        \n",
    "        # Train model\n",
    "        # Week 10/12 style: pass config file path instead of dict\n",
    "        # This ensures Model.from_config loads the config fresh and handles sequence_pad correctly\n",
    "        model = Model.from_config(str(config_path))\n",
    "        model.train(output_dir=str(model_dir))\n",
    "        \n",
    "        print(f\"    ✓ Training completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Training failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save progress\n",
    "results[\"training_completed\"] = True\n",
    "save_results(MODEL_TYPE, results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training completed for {MODEL_TYPE}\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inference - Aggregate Model on Each Subtype\n",
    "\n",
    "For the aggregate model, we need to run inference separately on each subtype's test sessions.\n",
    "This allows us to compare aggregate vs subtype-specific performance per subtype.\n",
    "\n",
    "### Aggregate Model - Inference on Fighting Subtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Model - Inference on Fighting Subtype\n",
    "# Run aggregate model inference on fighting subtype's test sessions\n",
    "\n",
    "AGGREGATE_MODEL_TYPE = \"aggregate\"\n",
    "TARGET_SUBTYPE = \"fighting\"\n",
    "\n",
    "# Load aggregate results\n",
    "agg_results = load_results(AGGREGATE_MODEL_TYPE) or {}\n",
    "agg_results.setdefault(\"subtype_results\", {})\n",
    "agg_results[\"subtype_results\"].setdefault(TARGET_SUBTYPE, {})\n",
    "\n",
    "# Load target subtype dataset to get test sessions\n",
    "if TARGET_SUBTYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {TARGET_SUBTYPE}\")\n",
    "\n",
    "# Week 12 style: Use preprocessed 2-class data for inference\n",
    "raw_target_dataset = dataset_configs[TARGET_SUBTYPE]\n",
    "target_dataset = DatasetConfig(\n",
    "    subtype=raw_target_dataset.subtype,\n",
    "    variant=raw_target_dataset.variant,\n",
    "    path=BINARY_DATA_ROOT / raw_target_dataset.subtype / raw_target_dataset.variant,\n",
    "    feature_count=raw_target_dataset.feature_count,\n",
    "    input_dir=\"features\",\n",
    ")\n",
    "populate_signal_indices(target_dataset)\n",
    "target_splits = parse_splits(target_dataset.split_file)\n",
    "\n",
    "# Load aggregate dataset config\n",
    "if AGGREGATE_MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {AGGREGATE_MODEL_TYPE}\")\n",
    "\n",
    "agg_dataset = dataset_configs[AGGREGATE_MODEL_TYPE]\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"AGGREGATE MODEL INFERENCE: {TARGET_SUBTYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test sessions: {len(target_splits['test'])}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already evaluated\n",
    "    if hparam_key in agg_results[\"subtype_results\"][TARGET_SUBTYPE]:\n",
    "        f1 = agg_results[\"subtype_results\"][TARGET_SUBTYPE][hparam_key].get(\"test_f1\", \"N/A\")\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already evaluated (F1={f1})\")\n",
    "        continue\n",
    "    \n",
    "    agg_model_dir = MODELS_ROOT / AGGREGATE_MODEL_TYPE / hparam_key\n",
    "    \n",
    "    if not (agg_model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✗ Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Running inference...\")\n",
    "    \n",
    "    try:\n",
    "        # Load aggregate model\n",
    "        model = Model.from_dir(str(agg_model_dir))\n",
    "        \n",
    "        # Run predictions on target subtype's test sessions\n",
    "        pred_dir = agg_model_dir / \"predictions\" / TARGET_SUBTYPE\n",
    "        pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for session in target_splits[\"test\"]:\n",
    "            model.predict(\n",
    "                data_path=str(target_dataset.path),  # Use target subtype's dataset path\n",
    "                input_dir=target_dataset.input_dir,\n",
    "                output_dir=str(pred_dir),\n",
    "                expt_ids=[session],\n",
    "            )\n",
    "        \n",
    "        # Load predictions and compute F1\n",
    "        from sklearn.metrics import f1_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        per_session_f1 = {}\n",
    "        \n",
    "        for session in target_splits[\"test\"]:\n",
    "            # Find prediction file\n",
    "            pred_file = None\n",
    "            for pattern in [f\"{session}_predictions.csv\", f\"{session}.csv\"]:\n",
    "                candidate = pred_dir / pattern\n",
    "                if candidate.exists():\n",
    "                    pred_file = candidate\n",
    "                    break\n",
    "            \n",
    "            if pred_file is None:\n",
    "                canonical = normalise_filename(session, TARGET_SUBTYPE)\n",
    "                feat_stem = target_dataset.feature_index.get(canonical, canonical)\n",
    "                pred_file = pred_dir / f\"{feat_stem}_predictions.csv\"\n",
    "            \n",
    "            label_file = target_dataset.labels_dir / f\"{session}.csv\"\n",
    "            if not label_file.exists():\n",
    "                canonical = normalise_filename(session, TARGET_SUBTYPE)\n",
    "                label_stem = target_dataset.label_index.get(canonical, canonical)\n",
    "                label_file = target_dataset.labels_dir / f\"{label_stem}.csv\"\n",
    "            \n",
    "            if not pred_file.exists() or not label_file.exists():\n",
    "                continue\n",
    "            \n",
    "            pred_df = pd.read_csv(pred_file)\n",
    "            label_df = pd.read_csv(label_file)\n",
    "            \n",
    "            length = min(len(pred_df), len(label_df))\n",
    "            if length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract predictions (2-class)\n",
    "            if \"predicted\" in pred_df.columns:\n",
    "                preds = pred_df[\"predicted\"].astype(int).values[:length]\n",
    "            elif \"flare\" in pred_df.columns:\n",
    "                preds = (pred_df[\"flare\"] > 0.5).astype(int).values[:length]\n",
    "            else:\n",
    "                preds = pred_df.iloc[:length, -1].astype(int).values\n",
    "            \n",
    "            # Extract labels (2-class)\n",
    "            if \"flare\" in label_df.columns:\n",
    "                labels = (label_df[\"flare\"] > 0).astype(int).values[:length]\n",
    "            else:\n",
    "                labels = (label_df.iloc[:length, -1] > 0).astype(int).values\n",
    "            \n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            # Per-session F1 (binary for 2-class: flare vs non-flare)\n",
    "            if labels.size > 0:\n",
    "                session_f1 = f1_score(labels, preds, average=\"binary\")\n",
    "                per_session_f1[session] = float(session_f1)\n",
    "        \n",
    "        # Overall F1 (per-session mean of binary F1)\n",
    "        if per_session_f1:\n",
    "            overall_f1 = float(np.nanmean(list(per_session_f1.values())))\n",
    "        else:\n",
    "            overall_f1 = float(\"nan\")\n",
    "        \n",
    "        agg_results[\"subtype_results\"][TARGET_SUBTYPE][hparam_key] = {\n",
    "            \"hparams\": {\n",
    "                \"num_hid_units\": int(num_hid_units),\n",
    "                \"num_layers\": int(num_layers),\n",
    "                \"num_lags\": int(num_lags),\n",
    "                \"lr\": float(lr),\n",
    "                \"num_epochs\": int(num_epochs),\n",
    "            },\n",
    "            \"test_f1\": float(overall_f1),\n",
    "            \"per_session_f1\": per_session_f1,\n",
    "            \"model_dir\": str(agg_model_dir),\n",
    "        }\n",
    "        \n",
    "        print(f\"    ✓ F1 = {overall_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_results(AGGREGATE_MODEL_TYPE, agg_results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Aggregate inference on {TARGET_SUBTYPE} completed\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Model - Inference on Hybrid, Ornamental, Wild Subtypes\n",
    "\n",
    "All inference cells for aggregate model on each subtype have been added below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Model - Inference on Hybrid Subtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Model - Inference on Hybrid Subtype\n",
    "# Run aggregate model inference on hybrid subtype's test sessions\n",
    "\n",
    "AGGREGATE_MODEL_TYPE = \"aggregate\"\n",
    "TARGET_SUBTYPE = \"hybrid\"\n",
    "\n",
    "# Load aggregate results\n",
    "agg_results = load_results(AGGREGATE_MODEL_TYPE) or {}\n",
    "agg_results.setdefault(\"subtype_results\", {})\n",
    "agg_results[\"subtype_results\"].setdefault(TARGET_SUBTYPE, {})\n",
    "\n",
    "# Load target subtype dataset to get test sessions\n",
    "if TARGET_SUBTYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {TARGET_SUBTYPE}\")\n",
    "\n",
    "# Week 12 style: Use preprocessed 2-class data for inference\n",
    "raw_target_dataset = dataset_configs[TARGET_SUBTYPE]\n",
    "target_dataset = DatasetConfig(\n",
    "    subtype=raw_target_dataset.subtype,\n",
    "    variant=raw_target_dataset.variant,\n",
    "    path=BINARY_DATA_ROOT / raw_target_dataset.subtype / raw_target_dataset.variant,\n",
    "    feature_count=raw_target_dataset.feature_count,\n",
    "    input_dir=\"features\",\n",
    ")\n",
    "populate_signal_indices(target_dataset)\n",
    "target_splits = parse_splits(target_dataset.split_file)\n",
    "\n",
    "# Load aggregate dataset config\n",
    "if AGGREGATE_MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {AGGREGATE_MODEL_TYPE}\")\n",
    "\n",
    "agg_dataset = dataset_configs[AGGREGATE_MODEL_TYPE]\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"AGGREGATE MODEL INFERENCE: {TARGET_SUBTYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test sessions: {len(target_splits['test'])}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already evaluated\n",
    "    if hparam_key in agg_results[\"subtype_results\"][TARGET_SUBTYPE]:\n",
    "        f1 = agg_results[\"subtype_results\"][TARGET_SUBTYPE][hparam_key].get(\"test_f1\", \"N/A\")\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already evaluated (F1={f1})\")\n",
    "        continue\n",
    "    \n",
    "    agg_model_dir = MODELS_ROOT / AGGREGATE_MODEL_TYPE / hparam_key\n",
    "    \n",
    "    if not (agg_model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✗ Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Running inference...\")\n",
    "    \n",
    "    try:\n",
    "        # Load aggregate model\n",
    "        model = Model.from_dir(str(agg_model_dir))\n",
    "        \n",
    "        # Run predictions on target subtype's test sessions\n",
    "        pred_dir = agg_model_dir / \"predictions\" / TARGET_SUBTYPE\n",
    "        pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for session in target_splits[\"test\"]:\n",
    "            model.predict(\n",
    "                data_path=str(target_dataset.path),  # Use target subtype's dataset path\n",
    "                input_dir=target_dataset.input_dir,\n",
    "                output_dir=str(pred_dir),\n",
    "                expt_ids=[session],\n",
    "            )\n",
    "        \n",
    "        # Load predictions and compute F1\n",
    "        from sklearn.metrics import f1_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        per_session_f1 = {}\n",
    "        \n",
    "        for session in target_splits[\"test\"]:\n",
    "            # Find prediction file\n",
    "            pred_file = None\n",
    "            for pattern in [f\"{session}_predictions.csv\", f\"{session}.csv\"]:\n",
    "                candidate = pred_dir / pattern\n",
    "                if candidate.exists():\n",
    "                    pred_file = candidate\n",
    "                    break\n",
    "            \n",
    "            if pred_file is None:\n",
    "                canonical = normalise_filename(session, TARGET_SUBTYPE)\n",
    "                feat_stem = target_dataset.feature_index.get(canonical, canonical)\n",
    "                pred_file = pred_dir / f\"{feat_stem}_predictions.csv\"\n",
    "            \n",
    "            label_file = target_dataset.labels_dir / f\"{session}.csv\"\n",
    "            if not label_file.exists():\n",
    "                canonical = normalise_filename(session, TARGET_SUBTYPE)\n",
    "                label_stem = target_dataset.label_index.get(canonical, canonical)\n",
    "                label_file = target_dataset.labels_dir / f\"{label_stem}.csv\"\n",
    "            \n",
    "            if not pred_file.exists() or not label_file.exists():\n",
    "                continue\n",
    "            \n",
    "            pred_df = pd.read_csv(pred_file)\n",
    "            label_df = pd.read_csv(label_file)\n",
    "            \n",
    "            length = min(len(pred_df), len(label_df))\n",
    "            if length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract predictions (2-class)\n",
    "            if \"predicted\" in pred_df.columns:\n",
    "                preds = pred_df[\"predicted\"].astype(int).values[:length]\n",
    "            elif \"flare\" in pred_df.columns:\n",
    "                preds = (pred_df[\"flare\"] > 0.5).astype(int).values[:length]\n",
    "            else:\n",
    "                preds = pred_df.iloc[:length, -1].astype(int).values\n",
    "            \n",
    "            # Extract labels (2-class)\n",
    "            if \"flare\" in label_df.columns:\n",
    "                labels = (label_df[\"flare\"] > 0).astype(int).values[:length]\n",
    "            else:\n",
    "                labels = (label_df.iloc[:length, -1] > 0).astype(int).values\n",
    "            \n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            # Per-session F1 (binary for 2-class: flare vs non-flare)\n",
    "            if labels.size > 0:\n",
    "                session_f1 = f1_score(labels, preds, average=\"binary\")\n",
    "                per_session_f1[session] = float(session_f1)\n",
    "        \n",
    "        # Overall F1 (per-session mean of binary F1)\n",
    "        if per_session_f1:\n",
    "            overall_f1 = float(np.nanmean(list(per_session_f1.values())))\n",
    "        else:\n",
    "            overall_f1 = float(\"nan\")\n",
    "        \n",
    "        agg_results[\"subtype_results\"][TARGET_SUBTYPE][hparam_key] = {\n",
    "            \"hparams\": {\n",
    "                \"num_hid_units\": int(num_hid_units),\n",
    "                \"num_layers\": int(num_layers),\n",
    "                \"num_lags\": int(num_lags),\n",
    "                \"lr\": float(lr),\n",
    "                \"num_epochs\": int(num_epochs),\n",
    "            },\n",
    "            \"test_f1\": float(overall_f1),\n",
    "            \"per_session_f1\": per_session_f1,\n",
    "            \"model_dir\": str(agg_model_dir),\n",
    "        }\n",
    "        \n",
    "        print(f\"    ✓ F1 = {overall_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_results(AGGREGATE_MODEL_TYPE, agg_results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Aggregate inference on {TARGET_SUBTYPE} completed\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Model - Inference on Ornamental Subtype\n",
    "# Run aggregate model inference on ornamental subtype's test sessions\n",
    "\n",
    "AGGREGATE_MODEL_TYPE = \"aggregate\"\n",
    "TARGET_SUBTYPE = \"ornamental\"\n",
    "\n",
    "# Load aggregate results\n",
    "agg_results = load_results(AGGREGATE_MODEL_TYPE) or {}\n",
    "agg_results.setdefault(\"subtype_results\", {})\n",
    "agg_results[\"subtype_results\"].setdefault(TARGET_SUBTYPE, {})\n",
    "\n",
    "# Load target subtype dataset to get test sessions\n",
    "if TARGET_SUBTYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {TARGET_SUBTYPE}\")\n",
    "\n",
    "# Week 12 style: Use preprocessed 2-class data for inference\n",
    "raw_target_dataset = dataset_configs[TARGET_SUBTYPE]\n",
    "target_dataset = DatasetConfig(\n",
    "    subtype=raw_target_dataset.subtype,\n",
    "    variant=raw_target_dataset.variant,\n",
    "    path=BINARY_DATA_ROOT / raw_target_dataset.subtype / raw_target_dataset.variant,\n",
    "    feature_count=raw_target_dataset.feature_count,\n",
    "    input_dir=\"features\",\n",
    ")\n",
    "populate_signal_indices(target_dataset)\n",
    "target_splits = parse_splits(target_dataset.split_file)\n",
    "\n",
    "# Load aggregate dataset config\n",
    "if AGGREGATE_MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {AGGREGATE_MODEL_TYPE}\")\n",
    "\n",
    "agg_dataset = dataset_configs[AGGREGATE_MODEL_TYPE]\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"AGGREGATE MODEL INFERENCE: {TARGET_SUBTYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test sessions: {len(target_splits['test'])}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already evaluated\n",
    "    if hparam_key in agg_results[\"subtype_results\"][TARGET_SUBTYPE]:\n",
    "        f1 = agg_results[\"subtype_results\"][TARGET_SUBTYPE][hparam_key].get(\"test_f1\", \"N/A\")\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already evaluated (F1={f1})\")\n",
    "        continue\n",
    "    \n",
    "    agg_model_dir = MODELS_ROOT / AGGREGATE_MODEL_TYPE / hparam_key\n",
    "    \n",
    "    if not (agg_model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✗ Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Running inference...\")\n",
    "    \n",
    "    try:\n",
    "        # Load aggregate model\n",
    "        model = Model.from_dir(str(agg_model_dir))\n",
    "        \n",
    "        # Run predictions on target subtype's test sessions\n",
    "        pred_dir = agg_model_dir / \"predictions\" / TARGET_SUBTYPE\n",
    "        pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for session in target_splits[\"test\"]:\n",
    "            # Resolve session alias to actual filename (e.g., \"2.7.1oR\" -> \"2.7.1R\" for ornamental)\n",
    "            resolved_session = resolve_session_alias(session, TARGET_SUBTYPE)\n",
    "            model.predict(\n",
    "                data_path=str(target_dataset.path),  # Use target subtype's dataset path\n",
    "                input_dir=target_dataset.input_dir,\n",
    "                output_dir=str(pred_dir),\n",
    "                expt_ids=[resolved_session], \n",
    "            )\n",
    "        \n",
    "        # Load predictions and compute F1\n",
    "        from sklearn.metrics import f1_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        per_session_f1 = {}\n",
    "        \n",
    "        for session in target_splits[\"test\"]:\n",
    "            # Find prediction file\n",
    "            pred_file = None\n",
    "            for pattern in [f\"{session}_predictions.csv\", f\"{session}.csv\"]:\n",
    "                candidate = pred_dir / pattern\n",
    "                if candidate.exists():\n",
    "                    pred_file = candidate\n",
    "                    break\n",
    "            \n",
    "            if pred_file is None:\n",
    "                canonical = normalise_filename(session, TARGET_SUBTYPE)\n",
    "                feat_stem = target_dataset.feature_index.get(canonical, canonical)\n",
    "                pred_file = pred_dir / f\"{feat_stem}_predictions.csv\"\n",
    "            \n",
    "            label_file = target_dataset.labels_dir / f\"{session}.csv\"\n",
    "            if not label_file.exists():\n",
    "                canonical = normalise_filename(session, TARGET_SUBTYPE)\n",
    "                label_stem = target_dataset.label_index.get(canonical, canonical)\n",
    "                label_file = target_dataset.labels_dir / f\"{label_stem}.csv\"\n",
    "            \n",
    "            if not pred_file.exists() or not label_file.exists():\n",
    "                continue\n",
    "            \n",
    "            pred_df = pd.read_csv(pred_file)\n",
    "            label_df = pd.read_csv(label_file)\n",
    "            \n",
    "            length = min(len(pred_df), len(label_df))\n",
    "            if length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract predictions (2-class)\n",
    "            if \"predicted\" in pred_df.columns:\n",
    "                preds = pred_df[\"predicted\"].astype(int).values[:length]\n",
    "            elif \"flare\" in pred_df.columns:\n",
    "                preds = (pred_df[\"flare\"] > 0.5).astype(int).values[:length]\n",
    "            else:\n",
    "                preds = pred_df.iloc[:length, -1].astype(int).values\n",
    "            \n",
    "            # Extract labels (2-class)\n",
    "            if \"flare\" in label_df.columns:\n",
    "                labels = (label_df[\"flare\"] > 0).astype(int).values[:length]\n",
    "            else:\n",
    "                labels = (label_df.iloc[:length, -1] > 0).astype(int).values\n",
    "            \n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            # Per-session F1 (binary for 2-class: flare vs non-flare)\n",
    "            if labels.size > 0:\n",
    "                session_f1 = f1_score(labels, preds, average=\"binary\")\n",
    "                per_session_f1[session] = float(session_f1)\n",
    "        \n",
    "        # Overall F1 (per-session mean of binary F1)\n",
    "        if per_session_f1:\n",
    "            overall_f1 = float(np.nanmean(list(per_session_f1.values())))\n",
    "        else:\n",
    "            overall_f1 = float(\"nan\")\n",
    "        \n",
    "        agg_results[\"subtype_results\"][TARGET_SUBTYPE][hparam_key] = {\n",
    "            \"hparams\": {\n",
    "                \"num_hid_units\": int(num_hid_units),\n",
    "                \"num_layers\": int(num_layers),\n",
    "                \"num_lags\": int(num_lags),\n",
    "                \"lr\": float(lr),\n",
    "                \"num_epochs\": int(num_epochs),\n",
    "            },\n",
    "            \"test_f1\": float(overall_f1),\n",
    "            \"per_session_f1\": per_session_f1,\n",
    "            \"model_dir\": str(agg_model_dir),\n",
    "        }\n",
    "        \n",
    "        print(f\"    ✓ F1 = {overall_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_results(AGGREGATE_MODEL_TYPE, agg_results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Aggregate inference on {TARGET_SUBTYPE} completed\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Model - Inference on Wild Subtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Model - Inference on Wild Subtype\n",
    "# Run aggregate model inference on wild subtype's test sessions\n",
    "\n",
    "AGGREGATE_MODEL_TYPE = \"aggregate\"\n",
    "TARGET_SUBTYPE = \"wild\"\n",
    "\n",
    "# Load aggregate results\n",
    "agg_results = load_results(AGGREGATE_MODEL_TYPE) or {}\n",
    "agg_results.setdefault(\"subtype_results\", {})\n",
    "agg_results[\"subtype_results\"].setdefault(TARGET_SUBTYPE, {})\n",
    "\n",
    "# Load target subtype dataset to get test sessions\n",
    "if TARGET_SUBTYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {TARGET_SUBTYPE}\")\n",
    "\n",
    "# Week 12 style: Use preprocessed 2-class data for inference\n",
    "raw_target_dataset = dataset_configs[TARGET_SUBTYPE]\n",
    "target_dataset = DatasetConfig(\n",
    "    subtype=raw_target_dataset.subtype,\n",
    "    variant=raw_target_dataset.variant,\n",
    "    path=BINARY_DATA_ROOT / raw_target_dataset.subtype / raw_target_dataset.variant,\n",
    "    feature_count=raw_target_dataset.feature_count,\n",
    "    input_dir=\"features\",\n",
    ")\n",
    "populate_signal_indices(target_dataset)\n",
    "target_splits = parse_splits(target_dataset.split_file)\n",
    "\n",
    "# Load aggregate dataset config\n",
    "if AGGREGATE_MODEL_TYPE not in dataset_configs:\n",
    "    raise ValueError(f\"Dataset config not found for {AGGREGATE_MODEL_TYPE}\")\n",
    "\n",
    "agg_dataset = dataset_configs[AGGREGATE_MODEL_TYPE]\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"AGGREGATE MODEL INFERENCE: {TARGET_SUBTYPE.upper()} SUBTYPE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test sessions: {len(target_splits['test'])}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for hparam_idx, hparams in enumerate(HPARAM_COMBINATIONS, 1):\n",
    "    num_hid_units, num_layers, num_lags, lr, num_epochs = hparams\n",
    "    hparam_key = f\"nh{num_hid_units}_ly{num_layers}_lg{num_lags}_lr{lr:.0e}\".replace(\"+\", \"\")\n",
    "    \n",
    "    # Skip if already evaluated\n",
    "    if hparam_key in agg_results[\"subtype_results\"][TARGET_SUBTYPE]:\n",
    "        f1 = agg_results[\"subtype_results\"][TARGET_SUBTYPE][hparam_key].get(\"test_f1\", \"N/A\")\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✓ Already evaluated (F1={f1})\")\n",
    "        continue\n",
    "    \n",
    "    agg_model_dir = MODELS_ROOT / AGGREGATE_MODEL_TYPE / hparam_key\n",
    "    \n",
    "    if not (agg_model_dir / \"final_model.ckpt\").exists():\n",
    "        print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} ✗ Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{hparam_idx:2d}/{len(HPARAM_COMBINATIONS)}] {hparam_key:30s} Running inference...\")\n",
    "    \n",
    "    try:\n",
    "        # Load aggregate model\n",
    "        model = Model.from_dir(str(agg_model_dir))\n",
    "        \n",
    "        # Run predictions on target subtype's test sessions\n",
    "        pred_dir = agg_model_dir / \"predictions\" / TARGET_SUBTYPE\n",
    "        pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for session in target_splits[\"test\"]:\n",
    "            model.predict(\n",
    "                data_path=str(target_dataset.path),  # Use target subtype's dataset path\n",
    "                input_dir=target_dataset.input_dir,\n",
    "                output_dir=str(pred_dir),\n",
    "                expt_ids=[session],\n",
    "            )\n",
    "        \n",
    "        # Load predictions and compute F1\n",
    "        from sklearn.metrics import f1_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        per_session_f1 = {}\n",
    "        \n",
    "        for session in target_splits[\"test\"]:\n",
    "            # Find prediction file\n",
    "            pred_file = None\n",
    "            for pattern in [f\"{session}_predictions.csv\", f\"{session}.csv\"]:\n",
    "                candidate = pred_dir / pattern\n",
    "                if candidate.exists():\n",
    "                    pred_file = candidate\n",
    "                    break\n",
    "            \n",
    "            if pred_file is None:\n",
    "                canonical = normalise_filename(session, TARGET_SUBTYPE)\n",
    "                feat_stem = target_dataset.feature_index.get(canonical, canonical)\n",
    "                pred_file = pred_dir / f\"{feat_stem}_predictions.csv\"\n",
    "            \n",
    "            label_file = target_dataset.labels_dir / f\"{session}.csv\"\n",
    "            if not label_file.exists():\n",
    "                canonical = normalise_filename(session, TARGET_SUBTYPE)\n",
    "                label_stem = target_dataset.label_index.get(canonical, canonical)\n",
    "                label_file = target_dataset.labels_dir / f\"{label_stem}.csv\"\n",
    "            \n",
    "            if not pred_file.exists() or not label_file.exists():\n",
    "                continue\n",
    "            \n",
    "            pred_df = pd.read_csv(pred_file)\n",
    "            label_df = pd.read_csv(label_file)\n",
    "            \n",
    "            length = min(len(pred_df), len(label_df))\n",
    "            if length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract predictions (2-class)\n",
    "            if \"predicted\" in pred_df.columns:\n",
    "                preds = pred_df[\"predicted\"].astype(int).values[:length]\n",
    "            elif \"flare\" in pred_df.columns:\n",
    "                preds = (pred_df[\"flare\"] > 0.5).astype(int).values[:length]\n",
    "            else:\n",
    "                preds = pred_df.iloc[:length, -1].astype(int).values\n",
    "            \n",
    "            # Extract labels (2-class)\n",
    "            if \"flare\" in label_df.columns:\n",
    "                labels = (label_df[\"flare\"] > 0).astype(int).values[:length]\n",
    "            else:\n",
    "                labels = (label_df.iloc[:length, -1] > 0).astype(int).values\n",
    "            \n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            # Per-session F1 (binary for 2-class: flare vs non-flare)\n",
    "            if labels.size > 0:\n",
    "                session_f1 = f1_score(labels, preds, average=\"binary\")\n",
    "                per_session_f1[session] = float(session_f1)\n",
    "        \n",
    "        # Overall F1 (per-session mean of binary F1)\n",
    "        if per_session_f1:\n",
    "            overall_f1 = float(np.nanmean(list(per_session_f1.values())))\n",
    "        else:\n",
    "            overall_f1 = float(\"nan\")\n",
    "        \n",
    "        agg_results[\"subtype_results\"][TARGET_SUBTYPE][hparam_key] = {\n",
    "            \"hparams\": {\n",
    "                \"num_hid_units\": int(num_hid_units),\n",
    "                \"num_layers\": int(num_layers),\n",
    "                \"num_lags\": int(num_lags),\n",
    "                \"lr\": float(lr),\n",
    "                \"num_epochs\": int(num_epochs),\n",
    "            },\n",
    "            \"test_f1\": float(overall_f1),\n",
    "            \"per_session_f1\": per_session_f1,\n",
    "            \"model_dir\": str(agg_model_dir),\n",
    "        }\n",
    "        \n",
    "        print(f\"    ✓ F1 = {overall_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_results(AGGREGATE_MODEL_TYPE, agg_results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Aggregate inference on {TARGET_SUBTYPE} completed\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: F1 Barplot and Flaring Rate Scatterplots\n",
    "\n",
    "Generate plots comparing type-specific vs aggregate models for each fish type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Results and Find Best Hyperparameters ---\n",
    "\n",
    "def load_all_results():\n",
    "    \"\"\"Load all subtype and aggregate results.\"\"\"\n",
    "    subtype_results = {}\n",
    "    for subtype in ALL_SUBTYPES:\n",
    "        results = load_results(subtype) or {}\n",
    "        subtype_results[subtype] = results.get(\"hparam_results\", {})\n",
    "    \n",
    "    aggregate_results = load_results(\"aggregate\") or {}\n",
    "    return subtype_results, aggregate_results\n",
    "\n",
    "def find_best_hparams(results_dict: Dict) -> Tuple[str, Dict]:\n",
    "    \"\"\"Find best hyperparameters based on test_f1.\"\"\"\n",
    "    if not results_dict:\n",
    "        return None, None\n",
    "    \n",
    "    best_key = None\n",
    "    best_f1 = -1.0\n",
    "    best_result = None\n",
    "    \n",
    "    for hparam_key, result in results_dict.items():\n",
    "        test_f1 = result.get(\"test_f1\", -1.0)\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            best_key = hparam_key\n",
    "            best_result = result\n",
    "    \n",
    "    return best_key, best_result\n",
    "\n",
    "def calculate_flaring_rate(subtype: str, session: str) -> float:\n",
    "    \"\"\"Calculate flaring rate from label file.\"\"\"\n",
    "    dataset_path = BINARY_DATA_ROOT / subtype / VARIANT\n",
    "    label_file = dataset_path / \"labels\" / f\"{session}.csv\"\n",
    "    \n",
    "    if not label_file.exists():\n",
    "        # Try with canonical name\n",
    "        canonical = normalise_filename(session, subtype)\n",
    "        label_file = dataset_path / \"labels\" / f\"{canonical}.csv\"\n",
    "    \n",
    "    if not label_file.exists():\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(label_file)\n",
    "    if \"flare\" in df.columns:\n",
    "        flare_rate = (df[\"flare\"] > 0).mean()\n",
    "    elif {\"background\", \"flare\"}.issubset(df.columns):\n",
    "        flare_rate = (df[\"flare\"] > df[\"background\"]).mean()\n",
    "    else:\n",
    "        # Use last column\n",
    "        flare_rate = (df.iloc[:, -1] > 0).mean()\n",
    "    \n",
    "    return float(flare_rate)\n",
    "\n",
    "# Load all results\n",
    "subtype_results, aggregate_results = load_all_results()\n",
    "\n",
    "# Find best hyperparameters for each subtype\n",
    "best_subtype_hparams = {}\n",
    "best_aggregate_hparams = {}\n",
    "\n",
    "for subtype in ALL_SUBTYPES:\n",
    "    best_key, best_result = find_best_hparams(subtype_results[subtype])\n",
    "    best_subtype_hparams[subtype] = (best_key, best_result)\n",
    "    \n",
    "    agg_subtype_results = aggregate_results.get(\"subtype_results\", {}).get(subtype, {})\n",
    "    best_agg_key, best_agg_result = find_best_hparams(agg_subtype_results)\n",
    "    best_aggregate_hparams[subtype] = (best_agg_key, best_agg_result)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "for subtype in ALL_SUBTYPES:\n",
    "    sub_key, sub_result = best_subtype_hparams[subtype]\n",
    "    agg_key, agg_result = best_aggregate_hparams[subtype]\n",
    "    sub_f1 = sub_result.get(\"test_f1\", \"N/A\") if sub_result else \"N/A\"\n",
    "    agg_f1 = agg_result.get(\"test_f1\", \"N/A\") if agg_result else \"N/A\"\n",
    "    print(f\"  {subtype}: subtype={sub_key} (F1={sub_f1:.4f}), aggregate={agg_key} (F1={agg_f1:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Data for Visualization ---\n",
    "\n",
    "# Collect F1 scores and flaring rates for each subtype\n",
    "plot_data = []\n",
    "\n",
    "for subtype in ALL_SUBTYPES:\n",
    "    # Get subtype-specific results\n",
    "    sub_key, sub_result = best_subtype_hparams[subtype]\n",
    "    if sub_result:\n",
    "        sub_per_session = sub_result.get(\"per_session_f1\", {})\n",
    "    else:\n",
    "        sub_per_session = {}\n",
    "    \n",
    "    # Get aggregate results for this subtype\n",
    "    agg_key, agg_result = best_aggregate_hparams[subtype]\n",
    "    if agg_result:\n",
    "        agg_per_session = agg_result.get(\"per_session_f1\", {})\n",
    "    else:\n",
    "        agg_per_session = {}\n",
    "    \n",
    "    # Get all test sessions (union of both)\n",
    "    all_sessions = set(sub_per_session.keys()) | set(agg_per_session.keys())\n",
    "    \n",
    "    for session in all_sessions:\n",
    "        # Calculate flaring rate\n",
    "        flare_rate = calculate_flaring_rate(subtype, session)\n",
    "        \n",
    "        plot_data.append({\n",
    "            \"subtype\": subtype,\n",
    "            \"session\": session,\n",
    "            \"subtype_f1\": sub_per_session.get(session, None),\n",
    "            \"aggregate_f1\": agg_per_session.get(session, None),\n",
    "            \"flare_rate\": flare_rate,\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Remove rows where both F1 scores are missing\n",
    "plot_df = plot_df[plot_df[\"subtype_f1\"].notna() | plot_df[\"aggregate_f1\"].notna()]\n",
    "\n",
    "print(f\"✓ Prepared data for {len(plot_df)} sessions across {len(ALL_SUBTYPES)} subtypes\")\n",
    "print(f\"\\nData summary:\")\n",
    "print(plot_df.groupby(\"subtype\")[[\"subtype_f1\", \"aggregate_f1\"]].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 1: F1 Barplot with Per-Video Points ---\n",
    "\n",
    "PLOTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Set up bar positions\n",
    "n_subtypes = len(ALL_SUBTYPES)\n",
    "bar_width = 0.35\n",
    "x_positions = np.arange(n_subtypes)\n",
    "x_subtype = x_positions - bar_width / 2\n",
    "x_aggregate = x_positions + bar_width / 2\n",
    "\n",
    "# Colors\n",
    "subtype_color = '#3498db'  # Blue\n",
    "aggregate_color = '#e74c3c'  # Red\n",
    "\n",
    "# Calculate means\n",
    "subtype_means = []\n",
    "aggregate_means = []\n",
    "\n",
    "for idx, subtype in enumerate(ALL_SUBTYPES):\n",
    "    subtype_data = plot_df[plot_df[\"subtype\"] == subtype]\n",
    "    \n",
    "    # Calculate means (using only non-NaN values)\n",
    "    sub_mean = subtype_data[\"subtype_f1\"].mean()\n",
    "    agg_mean = subtype_data[\"aggregate_f1\"].mean()\n",
    "    subtype_means.append(sub_mean)\n",
    "    aggregate_means.append(agg_mean)\n",
    "\n",
    "# Plot bars\n",
    "bars1 = ax.bar(x_subtype, subtype_means, bar_width, label='Type-Specific', color=subtype_color, alpha=0.7)\n",
    "bars2 = ax.bar(x_aggregate, aggregate_means, bar_width, label='Aggregate', color=aggregate_color, alpha=0.7)\n",
    "\n",
    "# Plot per-session points and connect lines\n",
    "for idx, subtype in enumerate(ALL_SUBTYPES):\n",
    "    subtype_data = plot_df[plot_df[\"subtype\"] == subtype]\n",
    "    \n",
    "    # Plot points and connect lines for each session\n",
    "    for _, row in subtype_data.iterrows():\n",
    "        sub_f1 = row[\"subtype_f1\"]\n",
    "        agg_f1 = row[\"aggregate_f1\"]\n",
    "        \n",
    "        # Plot points\n",
    "        if pd.notna(sub_f1):\n",
    "            ax.scatter(x_subtype[idx], sub_f1, color='darkblue', s=30, alpha=0.6, zorder=5)\n",
    "        if pd.notna(agg_f1):\n",
    "            ax.scatter(x_aggregate[idx], agg_f1, color='darkred', s=30, alpha=0.6, zorder=5)\n",
    "        \n",
    "        # Connect points if both exist (same session)\n",
    "        if pd.notna(sub_f1) and pd.notna(agg_f1):\n",
    "            ax.plot([x_subtype[idx], x_aggregate[idx]], [sub_f1, agg_f1], \n",
    "                   'gray', alpha=0.3, linewidth=0.5, zorder=1)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xlabel('Fish Type', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('F1 Score Comparison: Type-Specific vs Aggregate Models', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels([s.capitalize() for s in ALL_SUBTYPES])\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.0])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}',\n",
    "               ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_ROOT / \"f1_comparison_barplot.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved F1 barplot to {PLOTS_ROOT / 'f1_comparison_barplot.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 2: Flaring Rate Scatterplots (one per fish type) ---\n",
    "\n",
    "n_subtypes = len(ALL_SUBTYPES)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, subtype in enumerate(ALL_SUBTYPES):\n",
    "    ax = axes[idx]\n",
    "    subtype_data = plot_df[plot_df[\"subtype\"] == subtype].copy()\n",
    "    \n",
    "    # Filter out rows with missing flare_rate\n",
    "    subtype_data = subtype_data[subtype_data[\"flare_rate\"].notna()]\n",
    "    \n",
    "    if len(subtype_data) == 0:\n",
    "        ax.text(0.5, 0.5, f'No data for {subtype}', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'{subtype.capitalize()}', fontsize=12, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    # Plot scatter points\n",
    "    sub_mask = subtype_data[\"subtype_f1\"].notna()\n",
    "    agg_mask = subtype_data[\"aggregate_f1\"].notna()\n",
    "    \n",
    "    if sub_mask.any():\n",
    "        ax.scatter(subtype_data.loc[sub_mask, \"flare_rate\"], \n",
    "                  subtype_data.loc[sub_mask, \"subtype_f1\"],\n",
    "                  label='Type-Specific', color='#3498db', s=60, alpha=0.7, edgecolors='darkblue', linewidth=1)\n",
    "    \n",
    "    if agg_mask.any():\n",
    "        ax.scatter(subtype_data.loc[agg_mask, \"flare_rate\"], \n",
    "                  subtype_data.loc[agg_mask, \"aggregate_f1\"],\n",
    "                  label='Aggregate', color='#e74c3c', s=60, alpha=0.7, edgecolors='darkred', linewidth=1, marker='^')\n",
    "    \n",
    "    # Connect points for same session\n",
    "    for _, row in subtype_data.iterrows():\n",
    "        if pd.notna(row[\"subtype_f1\"]) and pd.notna(row[\"aggregate_f1\"]):\n",
    "            ax.plot([row[\"flare_rate\"], row[\"flare_rate\"]], \n",
    "                   [row[\"subtype_f1\"], row[\"aggregate_f1\"]],\n",
    "                   'gray', alpha=0.3, linewidth=1, zorder=0)\n",
    "    \n",
    "    # Customize subplot\n",
    "    ax.set_xlabel('Flaring Rate', fontsize=10, fontweight='bold')\n",
    "    ax.set_ylabel('F1 Score', fontsize=10, fontweight='bold')\n",
    "    ax.set_title(f'{subtype.capitalize()}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1.0])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "\n",
    "plt.suptitle('F1 Score vs Flaring Rate: Type-Specific vs Aggregate Models', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_ROOT / \"flaring_rate_scatterplots.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved flaring rate scatterplots to {PLOTS_ROOT / 'flaring_rate_scatterplots.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization v2 (per mentor request)\n",
    "\n",
    "- Set **zero flare-rate** sessions to **NaN** (excluded from F1 means/points)\n",
    "- Select **one single aggregate model** by taking the mean F1 across the 4 fish types (evenly weighted)\n",
    "- Replot using that single aggregate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization v2: choose single aggregate model + set zero-flare sessions to NaN ---\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _read_json(path: Path) -> dict:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _best_hparam_key(hparam_results: dict) -> str | None:\n",
    "    if not hparam_results:\n",
    "        return None\n",
    "    best_key = None\n",
    "    best_f1 = -1e18\n",
    "    for k, v in hparam_results.items():\n",
    "        f1 = v.get(\"test_f1\", None)\n",
    "        if f1 is None:\n",
    "            continue\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_key = k\n",
    "    return best_key\n",
    "\n",
    "\n",
    "def _safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def calculate_flaring_rate(subtype: str, session: str) -> float | None:\n",
    "    \"\"\"Flaring rate = fraction of frames labeled as flare in the 2-class labels.\"\"\"\n",
    "    labels_dir = (BINARY_DATA_ROOT / subtype / VARIANT) / \"labels\"\n",
    "    label_file = _find_label_file(labels_dir, session, subtype)\n",
    "\n",
    "    if label_file is None:\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(label_file)\n",
    "    if \"flare\" in df.columns:\n",
    "        return float((df[\"flare\"] > 0).mean())\n",
    "    if {\"background\", \"flare\"}.issubset(df.columns):\n",
    "        return float((df[\"flare\"] > df[\"background\"]).mean())\n",
    "    return float((df.iloc[:, -1] > 0).mean())\n",
    "\n",
    "\n",
    "# ---- Fallbacks (in case kernel restarted and earlier config cells weren't run) ----\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path(\"C:/Columbia Capstone\"))\n",
    "WEEK14_ROOT = globals().get(\"WEEK14_ROOT\", PROJECT_ROOT / \"Week 14\")\n",
    "HPARAM_ROOT = globals().get(\"HPARAM_ROOT\", WEEK14_ROOT / \"hyperparameter_sweep\")\n",
    "\n",
    "RESULTS_ROOT = globals().get(\"RESULTS_ROOT\", HPARAM_ROOT / \"results\")\n",
    "PLOTS_ROOT = globals().get(\"PLOTS_ROOT\", HPARAM_ROOT / \"plots\")\n",
    "BINARY_DATA_ROOT = globals().get(\"BINARY_DATA_ROOT\", HPARAM_ROOT / \"binary_data\")\n",
    "\n",
    "VARIANT = globals().get(\"VARIANT\", \"LP_with_cal_contour\")\n",
    "ALL_SUBTYPES = globals().get(\"ALL_SUBTYPES\", [\"fighting\", \"hybrid\", \"ornamental\", \"wild\"])\n",
    "\n",
    "PLOTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _find_label_file(labels_dir: Path, session: str, subtype: str) -> Path | None:\n",
    "    candidates = [labels_dir / f\"{session}.csv\"]\n",
    "\n",
    "    # common alias: ornamental uses \"oR\" suffix in some places\n",
    "    if subtype == \"ornamental\" and session.endswith(\"oR\"):\n",
    "        candidates.append(labels_dir / f\"{session[:-2]}R.csv\")\n",
    "\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # fallback: prefix match (matches e.g. session_labels.csv)\n",
    "    matches = list(labels_dir.glob(f\"{session}*.csv\"))\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "\n",
    "# Load results\n",
    "results_dir = RESULTS_ROOT\n",
    "subtype_results = {\n",
    "    st: _read_json(results_dir / f\"{st}_results.json\").get(\"hparam_results\", {})\n",
    "    for st in ALL_SUBTYPES\n",
    "}\n",
    "aggregate_results = _read_json(results_dir / \"aggregate_results.json\").get(\"subtype_results\", {})\n",
    "\n",
    "# Best subtype-specific model per fish type (as before)\n",
    "best_subtype_key = {st: _best_hparam_key(subtype_results[st]) for st in ALL_SUBTYPES}\n",
    "\n",
    "# Choose ONE aggregate model globally: mean across fish types (evenly weighted)\n",
    "agg_keys_sets = []\n",
    "for st in ALL_SUBTYPES:\n",
    "    st_dict = aggregate_results.get(st, {})\n",
    "    agg_keys_sets.append(set(st_dict.keys()))\n",
    "\n",
    "common_agg_keys = set.intersection(*agg_keys_sets) if agg_keys_sets else set()\n",
    "if not common_agg_keys:\n",
    "    raise RuntimeError(\"No common aggregate hparam keys found across all fish types.\")\n",
    "\n",
    "agg_key_to_mean = {}\n",
    "for k in sorted(common_agg_keys):\n",
    "    f1s = []\n",
    "    ok = True\n",
    "    for st in ALL_SUBTYPES:\n",
    "        v = aggregate_results.get(st, {}).get(k, {})\n",
    "        f1 = v.get(\"test_f1\", None)\n",
    "        if f1 is None:\n",
    "            ok = False\n",
    "            break\n",
    "        f1s.append(_safe_float(f1))\n",
    "    if ok and all(np.isfinite(f1s)):\n",
    "        agg_key_to_mean[k] = float(np.mean(f1s))\n",
    "\n",
    "if not agg_key_to_mean:\n",
    "    raise RuntimeError(\"Could not compute mean F1 for any aggregate model across fish types.\")\n",
    "\n",
    "global_aggregate_key = max(agg_key_to_mean, key=lambda kk: agg_key_to_mean[kk])\n",
    "\n",
    "print(\"Selected models:\")\n",
    "for st in ALL_SUBTYPES:\n",
    "    print(f\"  {st}: subtype_best={best_subtype_key[st]}\")\n",
    "print(f\"  aggregate_single_best={global_aggregate_key} (mean across fish types={agg_key_to_mean[global_aggregate_key]:.4f})\")\n",
    "\n",
    "\n",
    "# Build plotting dataframe\n",
    "rows = []\n",
    "EPS = 1e-12\n",
    "\n",
    "for st in ALL_SUBTYPES:\n",
    "    sub_key = best_subtype_key[st]\n",
    "    sub_per = (subtype_results.get(st, {}).get(sub_key, {}) if sub_key else {}).get(\"per_session_f1\", {})\n",
    "\n",
    "    agg_per = (\n",
    "        aggregate_results.get(st, {}).get(global_aggregate_key, {}).get(\"per_session_f1\", {})\n",
    "    )\n",
    "\n",
    "    sessions = set(sub_per.keys()) | set(agg_per.keys())\n",
    "\n",
    "    for session in sessions:\n",
    "        flare_rate = calculate_flaring_rate(st, session)\n",
    "        sub_f1 = sub_per.get(session, np.nan)\n",
    "        agg_f1 = agg_per.get(session, np.nan)\n",
    "\n",
    "        # If flare_rate is zero → set F1 to NaN (excluded from barplot & points)\n",
    "        if flare_rate is not None and flare_rate <= EPS:\n",
    "            sub_f1 = np.nan\n",
    "            agg_f1 = np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"subtype\": st,\n",
    "                \"session\": session,\n",
    "                \"flare_rate\": flare_rate,\n",
    "                \"subtype_f1\": _safe_float(sub_f1),\n",
    "                \"aggregate_f1\": _safe_float(agg_f1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "plot_df_v2 = pd.DataFrame(rows)\n",
    "\n",
    "# Drop rows where both are NaN\n",
    "plot_df_v2 = plot_df_v2[plot_df_v2[\"subtype_f1\"].notna() | plot_df_v2[\"aggregate_f1\"].notna()].copy()\n",
    "\n",
    "print(\"\\nCounts (after excluding zero-flare sessions):\")\n",
    "print(plot_df_v2.groupby(\"subtype\")[[\"subtype_f1\", \"aggregate_f1\"]].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot v2: F1 barplot (zero-flare excluded) + single aggregate model ---\n",
    "\n",
    "PLOTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "bar_width = 0.35\n",
    "x_positions = np.arange(len(ALL_SUBTYPES))\n",
    "x_subtype = x_positions - bar_width / 2\n",
    "x_aggregate = x_positions + bar_width / 2\n",
    "\n",
    "subtype_color = \"#3498db\"\n",
    "aggregate_color = \"#e74c3c\"\n",
    "\n",
    "subtype_means = []\n",
    "aggregate_means = []\n",
    "\n",
    "for st in ALL_SUBTYPES:\n",
    "    d = plot_df_v2[plot_df_v2[\"subtype\"] == st]\n",
    "    subtype_means.append(float(d[\"subtype_f1\"].mean()))\n",
    "    aggregate_means.append(float(d[\"aggregate_f1\"].mean()))\n",
    "\n",
    "bars1 = ax.bar(x_subtype, subtype_means, bar_width, label=\"Type-Specific\", color=subtype_color, alpha=0.7)\n",
    "bars2 = ax.bar(x_aggregate, aggregate_means, bar_width, label=\"Aggregate (single model)\", color=aggregate_color, alpha=0.7)\n",
    "\n",
    "# points + connecting lines per session\n",
    "for idx, st in enumerate(ALL_SUBTYPES):\n",
    "    d = plot_df_v2[plot_df_v2[\"subtype\"] == st]\n",
    "    for _, row in d.iterrows():\n",
    "        sub_f1 = row[\"subtype_f1\"]\n",
    "        agg_f1 = row[\"aggregate_f1\"]\n",
    "        if pd.notna(sub_f1):\n",
    "            ax.scatter(x_subtype[idx], sub_f1, color=\"darkblue\", s=30, alpha=0.65, zorder=5)\n",
    "        if pd.notna(agg_f1):\n",
    "            ax.scatter(x_aggregate[idx], agg_f1, color=\"darkred\", s=30, alpha=0.65, zorder=5)\n",
    "        if pd.notna(sub_f1) and pd.notna(agg_f1):\n",
    "            ax.plot([x_subtype[idx], x_aggregate[idx]], [sub_f1, agg_f1], color=\"gray\", alpha=0.3, linewidth=0.6, zorder=1)\n",
    "\n",
    "ax.set_xlabel(\"Fish Type\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"F1 Score\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"F1 Comparison (zero-flare excluded)\\nAggregate uses single model: {global_aggregate_key}\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels([s.capitalize() for s in ALL_SUBTYPES])\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "for bars in (bars1, bars2):\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        if np.isfinite(h):\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2.0, h, f\"{h:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "out_path = PLOTS_ROOT / \"f1_comparison_barplot_v2_single_agg_zero_flare_nan.png\"\n",
    "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✓ Saved: {out_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot v2: F1 vs flaring-rate scatterplots (zero-flare excluded) ---\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, st in enumerate(ALL_SUBTYPES):\n",
    "    ax = axes[idx]\n",
    "    d = plot_df_v2[plot_df_v2[\"subtype\"] == st].copy()\n",
    "    d = d[d[\"flare_rate\"].notna()]\n",
    "\n",
    "    if d.empty:\n",
    "        ax.text(0.5, 0.5, f\"No data for {st}\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        ax.set_title(st.capitalize())\n",
    "        continue\n",
    "\n",
    "    sub_mask = d[\"subtype_f1\"].notna()\n",
    "    agg_mask = d[\"aggregate_f1\"].notna()\n",
    "\n",
    "    if sub_mask.any():\n",
    "        ax.scatter(\n",
    "            d.loc[sub_mask, \"flare_rate\"],\n",
    "            d.loc[sub_mask, \"subtype_f1\"],\n",
    "            label=\"Type-Specific\",\n",
    "            color=\"#3498db\",\n",
    "            s=60,\n",
    "            alpha=0.75,\n",
    "            edgecolors=\"darkblue\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "    if agg_mask.any():\n",
    "        ax.scatter(\n",
    "            d.loc[agg_mask, \"flare_rate\"],\n",
    "            d.loc[agg_mask, \"aggregate_f1\"],\n",
    "            label=\"Aggregate (single model)\",\n",
    "            color=\"#e74c3c\",\n",
    "            s=60,\n",
    "            alpha=0.75,\n",
    "            edgecolors=\"darkred\",\n",
    "            linewidth=1,\n",
    "            marker=\"^\",\n",
    "        )\n",
    "\n",
    "    # connect same session\n",
    "    for _, row in d.iterrows():\n",
    "        if pd.notna(row[\"subtype_f1\"]) and pd.notna(row[\"aggregate_f1\"]):\n",
    "            ax.plot(\n",
    "                [row[\"flare_rate\"], row[\"flare_rate\"]],\n",
    "                [row[\"subtype_f1\"], row[\"aggregate_f1\"]],\n",
    "                color=\"gray\",\n",
    "                alpha=0.3,\n",
    "                linewidth=1,\n",
    "                zorder=0,\n",
    "            )\n",
    "\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel(\"Flaring Rate\")\n",
    "    ax.set_ylabel(\"F1 Score\")\n",
    "    ax.set_title(st.capitalize())\n",
    "    ax.legend(loc=\"best\", fontsize=9)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"F1 vs Flaring Rate (zero-flare excluded)\\nAggregate uses single model: {global_aggregate_key}\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.995,\n",
    ")\n",
    "plt.tight_layout()\n",
    "out_path = PLOTS_ROOT / \"flaring_rate_scatterplots_v2_single_agg_zero_flare_nan.png\"\n",
    "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✓ Saved: {out_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Aggregate Training (ALL data)\n",
    "\n",
    "Train the final aggregate model using **all sessions** (train + test + val across all subtypes) with the selected best aggregate hyperparameters:\n",
    "\n",
    "- `num_hid_units=64`\n",
    "- `num_layers=3`\n",
    "- `num_lags=8`\n",
    "- `lr=5e-4`\n",
    "\n",
    "Outputs are saved to:\n",
    "- `Week 14/hyperparameter_sweep/models/aggregate/<hparam_key>__all_data_final/`\n",
    "- `Week 14/deliverables/models/aggregate_all_data/<hparam_key>/` (minimal copy for packaging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train final aggregate model on ALL data and copy artifacts into deliverables/ ---\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "from lightning_action.api import Model\n",
    "\n",
    "\n",
    "def _find_repo_root() -> Path:\n",
    "    \"\"\"Find repo root by locating 'Week 14/hyperparameter_sweep'.\"\"\"\n",
    "    for base in [Path.cwd(), Path.cwd().resolve()]:\n",
    "        for p in [base] + list(base.parents):\n",
    "            if (p / \"Week 14\" / \"hyperparameter_sweep\").exists():\n",
    "                return p\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "\n",
    "def _parse_splits(path: Path) -> dict[str, list[str]]:\n",
    "    current = None\n",
    "    splits: dict[str, list[str]] = {\"train\": [], \"test\": [], \"val\": []}\n",
    "    for raw_line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = raw_line.split(\"#\", 1)[0].strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.endswith(\":\"):\n",
    "            key = line[:-1].strip()\n",
    "            current = key if key in splits else None\n",
    "            continue\n",
    "        if current and line.startswith(\"-\"):\n",
    "            value = line[1:].strip()\n",
    "            if value:\n",
    "                splits[current].append(value)\n",
    "    return splits\n",
    "\n",
    "\n",
    "# --- Config ---\n",
    "REPO_ROOT = _find_repo_root()\n",
    "WEEK14_ROOT = REPO_ROOT / \"Week 14\"\n",
    "SWEEP_ROOT = WEEK14_ROOT / \"hyperparameter_sweep\"\n",
    "\n",
    "VARIANT = \"LP_with_cal_contour\"\n",
    "HPARAM_KEY = \"nh64_ly3_lg8_lr5e-04\"\n",
    "\n",
    "NUM_HID_UNITS = 64\n",
    "NUM_LAYERS = 3\n",
    "NUM_LAGS = 8\n",
    "LR = 5e-4\n",
    "NUM_EPOCHS = 400\n",
    "\n",
    "INPUT_SIZE = 18\n",
    "OUTPUT_SIZE = 2\n",
    "SEED = 43\n",
    "\n",
    "DATA_PATH = SWEEP_ROOT / \"binary_data\" / \"aggregate\" / VARIANT\n",
    "SPLITS = _parse_splits(DATA_PATH / \"splits.yaml\")\n",
    "EXPT_IDS = SPLITS[\"train\"] + SPLITS[\"test\"] + SPLITS.get(\"val\", [])\n",
    "\n",
    "OUT_DIR = SWEEP_ROOT / \"models\" / \"aggregate\" / f\"{HPARAM_KEY}__all_data_final\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build lightning-action config (do not set training.sequence_pad)\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"data_path\": str(DATA_PATH),\n",
    "        \"input_dir\": \"features\",\n",
    "        \"transforms\": [\"ZScore\"],\n",
    "        \"expt_ids\": EXPT_IDS,\n",
    "        \"seed\": SEED,\n",
    "        \"ignore_index\": -100,\n",
    "        \"weight_classes\": True,\n",
    "        \"label_names\": [\"background\", \"flare\"],\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"backbone\": \"dtcn\",\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"output_size\": OUTPUT_SIZE,\n",
    "        \"num_hid_units\": NUM_HID_UNITS,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"num_lags\": NUM_LAGS,\n",
    "        \"seed\": SEED,\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"lr\": LR,\n",
    "        \"wd\": 0,\n",
    "        \"scheduler\": None,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"gpu\",\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": 16,\n",
    "        \"num_workers\": 4,\n",
    "        \"sequence_length\": 1000,\n",
    "        \"train_probability\": 0.95,\n",
    "        \"val_probability\": 0.05,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save config.yaml for reproducibility\n",
    "with open(OUT_DIR / \"config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(config, f, sort_keys=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL AGGREGATE TRAINING (ALL DATA)\")\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Data: {DATA_PATH}\")\n",
    "print(f\"Sessions (all): {len(EXPT_IDS)}\")\n",
    "print(f\"Hyperparams: {HPARAM_KEY} (nh=64, ly=3, lg=8, lr=5e-4)\")\n",
    "print(f\"Output: {OUT_DIR}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train\n",
    "model = Model.from_config(config)\n",
    "# post_inference=False to avoid generating predictions for all 77 sessions during training\n",
    "model.train(output_dir=OUT_DIR, post_inference=False)\n",
    "\n",
    "# Copy minimal artifacts into deliverables/\n",
    "DELIV_DIR = WEEK14_ROOT / \"deliverables\" / \"models\" / \"aggregate_all_data\" / HPARAM_KEY\n",
    "DELIV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Always copy config\n",
    "shutil.copy2(OUT_DIR / \"config.yaml\", DELIV_DIR / \"config.yaml\")\n",
    "\n",
    "# Copy final_model.ckpt if present; otherwise copy best checkpoint if available\n",
    "final_ckpt = OUT_DIR / \"final_model.ckpt\"\n",
    "if final_ckpt.exists():\n",
    "    shutil.copy2(final_ckpt, DELIV_DIR / \"final_model.ckpt\")\n",
    "else:\n",
    "    ckpts = list((OUT_DIR / \"tb_logs\").rglob(\"*.ckpt\"))\n",
    "    if ckpts:\n",
    "        shutil.copy2(ckpts[0], DELIV_DIR / ckpts[0].name)\n",
    "\n",
    "# Write a small pointer JSON as well\n",
    "pointer = {\n",
    "    \"hparam_key\": HPARAM_KEY,\n",
    "    \"variant\": VARIANT,\n",
    "    \"train_split\": \"all (train+test+val)\",\n",
    "    \"data_path\": str(DATA_PATH),\n",
    "    \"model_dir\": str(OUT_DIR),\n",
    "    \"deliverables_copy\": str(DELIV_DIR),\n",
    "}\n",
    "with open(DELIV_DIR / \"MODEL_POINTER.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pointer, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Deliverables copy saved to: {DELIV_DIR}\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Aggregate Model — True vs Predicted Flare Rate (ALL sessions)\n",
    "\n",
    "This plot uses the **final aggregate model** trained on **all sessions** and compares:\n",
    "- **x-axis**: ground-truth flare rate (from labels)\n",
    "- **y-axis**: predicted flare rate (from model predictions)\n",
    "\n",
    "It is shown **split by fish type** (fighting / hybrid / ornamental / wild) over **all 77 sessions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scatterplots: True vs Predicted flare rate for FINAL aggregate (all sessions) ---\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightning_action.api import Model\n",
    "\n",
    "\n",
    "def _find_repo_root() -> Path:\n",
    "    for base in [Path.cwd(), Path.cwd().resolve()]:\n",
    "        for p in [base] + list(base.parents):\n",
    "            if (p / \"Week 14\" / \"hyperparameter_sweep\").exists():\n",
    "                return p\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "\n",
    "def _parse_splits(path: Path) -> dict[str, list[str]]:\n",
    "    current = None\n",
    "    splits: dict[str, list[str]] = {\"train\": [], \"test\": [], \"val\": []}\n",
    "    for raw_line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = raw_line.split(\"#\", 1)[0].strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.endswith(\":\"):\n",
    "            key = line[:-1].strip()\n",
    "            current = key if key in splits else None\n",
    "            continue\n",
    "        if current and line.startswith(\"-\"):\n",
    "            value = line[1:].strip()\n",
    "            if value:\n",
    "                splits[current].append(value)\n",
    "    return splits\n",
    "\n",
    "\n",
    "def _true_flare_rate(label_file: Path) -> float:\n",
    "    df = pd.read_csv(label_file)\n",
    "    if \"flare\" in df.columns:\n",
    "        return float((df[\"flare\"] > 0).mean())\n",
    "    if {\"background\", \"flare\"}.issubset(df.columns):\n",
    "        return float((df[\"flare\"] > df[\"background\"]).mean())\n",
    "    return float((df.iloc[:, -1] > 0).mean())\n",
    "\n",
    "\n",
    "def _pred_flare_rate(pred_file: Path) -> float:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    if {\"background\", \"flare\"}.issubset(df.columns):\n",
    "        preds = np.argmax(df[[\"background\", \"flare\"]].values, axis=1)\n",
    "        return float(preds.mean())\n",
    "    if \"predicted\" in df.columns:\n",
    "        vals = df[\"predicted\"].values\n",
    "        # treat >0 as flare\n",
    "        return float((vals > 0).mean())\n",
    "    if \"flare\" in df.columns:\n",
    "        # probability threshold\n",
    "        return float((df[\"flare\"] > 0.5).mean())\n",
    "    return float((df.iloc[:, -1] > 0).mean())\n",
    "\n",
    "\n",
    "REPO_ROOT = _find_repo_root()\n",
    "WEEK14_ROOT = REPO_ROOT / \"Week 14\"\n",
    "SWEEP_ROOT = WEEK14_ROOT / \"hyperparameter_sweep\"\n",
    "PLOTS_ROOT = SWEEP_ROOT / \"plots\"\n",
    "PLOTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DELIV_PLOTS = WEEK14_ROOT / \"deliverables\" / \"plots\" / \"final_aggregate_all_data\"\n",
    "DELIV_PLOTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "VARIANT = \"LP_with_cal_contour\"\n",
    "HPARAM_KEY = \"nh64_ly3_lg8_lr5e-04\"\n",
    "\n",
    "DATA_PATH = SWEEP_ROOT / \"binary_data\" / \"aggregate\" / VARIANT\n",
    "splits = _parse_splits(DATA_PATH / \"splits.yaml\")\n",
    "expt_ids = splits[\"train\"] + splits[\"test\"] + splits.get(\"val\", [])\n",
    "\n",
    "MODEL_DIR = SWEEP_ROOT / \"models\" / \"aggregate\" / f\"{HPARAM_KEY}__all_data_final\"\n",
    "if not MODEL_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Final model dir not found: {MODEL_DIR}\")\n",
    "\n",
    "# Predictions for ALL sessions\n",
    "PRED_DIR = MODEL_DIR / \"predictions_all_sessions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "missing = [eid for eid in expt_ids if not (PRED_DIR / f\"{eid}_predictions.csv\").exists()]\n",
    "if missing:\n",
    "    print(f\"Missing {len(missing)} prediction files. Running inference now...\")\n",
    "    model = Model.from_dir(MODEL_DIR)\n",
    "    model.predict(\n",
    "        data_path=str(DATA_PATH),\n",
    "        input_dir=\"features\",\n",
    "        output_dir=str(PRED_DIR),\n",
    "        expt_ids=missing,\n",
    "    )\n",
    "else:\n",
    "    print(\"All prediction files already exist.\")\n",
    "\n",
    "# Compute true vs predicted flare rate\n",
    "rows = []\n",
    "labels_dir = DATA_PATH / \"labels\"\n",
    "\n",
    "for eid in expt_ids:\n",
    "    label_file = labels_dir / f\"{eid}.csv\"\n",
    "    pred_file = PRED_DIR / f\"{eid}_predictions.csv\"\n",
    "    if not label_file.exists() or not pred_file.exists():\n",
    "        continue\n",
    "\n",
    "    fish_type = eid.split(\"__\", 1)[0] if \"__\" in eid else \"unknown\"\n",
    "    session = eid.split(\"__\", 1)[1] if \"__\" in eid else eid\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"fish_type\": fish_type,\n",
    "            \"session\": session,\n",
    "            \"expt_id\": eid,\n",
    "            \"true_flare_rate\": _true_flare_rate(label_file),\n",
    "            \"pred_flare_rate\": _pred_flare_rate(pred_file),\n",
    "        }\n",
    "    )\n",
    "\n",
    "rate_df = pd.DataFrame(rows)\n",
    "print(rate_df.groupby(\"fish_type\")[[\"true_flare_rate\", \"pred_flare_rate\"]].describe())\n",
    "\n",
    "# Plot: split by fish type\n",
    "fish_types = [\"fighting\", \"hybrid\", \"ornamental\", \"wild\"]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ft in enumerate(fish_types):\n",
    "    ax = axes[i]\n",
    "    d = rate_df[rate_df[\"fish_type\"] == ft]\n",
    "    ax.scatter(d[\"true_flare_rate\"], d[\"pred_flare_rate\"], s=60, alpha=0.8)\n",
    "\n",
    "    # y=x reference\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", linewidth=1)\n",
    "\n",
    "    # labels\n",
    "    for _, r in d.iterrows():\n",
    "        ax.annotate(r[\"session\"], (r[\"true_flare_rate\"], r[\"pred_flare_rate\"]), fontsize=8, alpha=0.7)\n",
    "\n",
    "    ax.set_title(ft.capitalize())\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel(\"True flare rate\")\n",
    "    ax.set_ylabel(\"Predicted flare rate\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Final aggregate (all-data) — True vs Predicted flare rate\\n{HPARAM_KEY}\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_png = PLOTS_ROOT / f\"final_aggregate_all_data_true_vs_pred_flare_rate__{HPARAM_KEY}.png\"\n",
    "plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Copy to deliverables\n",
    "shutil.copy2(out_png, DELIV_PLOTS / out_png.name)\n",
    "print(f\"\\n✓ Saved plot: {out_png}\")\n",
    "print(f\"✓ Copied to: {DELIV_PLOTS / out_png.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}